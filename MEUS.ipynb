{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024362cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import joblib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf # Importado para tf.random.set_seed\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold, # No se usa directamente, GridSearchCV lo maneja\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# --- CONFIGURACIÓN DE HILOS DE TENSORFLOW ---\n",
    "\n",
    "try:\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    logging.info(\"TensorFlow inter-op parallelism threads set to 1.\")\n",
    "    logging.info(\"TensorFlow intra-op parallelism threads set to 1.\")\n",
    "except RuntimeError as e:\n",
    "    # Esto puede ocurrir si los hilos ya fueron configurados (ej. en un entorno interactivo como Jupyter)\n",
    "    logging.warning(f\"Could not set TensorFlow threading: {e}. This might be okay if already configured.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuración global\n",
    "# -------------------------------------------------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "LOG_FMT = \"%(asctime)s %(levelname)-8s %(message)s\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba75b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:53:36,836 INFO     =================================================\n",
      "2025-05-23 15:53:36,836 INFO     === Iniciando experimento: MEUS_MajClass_Scaling_scenario1 ===\n",
      "2025-05-23 15:53:36,837 INFO     Técnica: MEUS (Mahalanobis Undersampling en Clase Mayoritaria con Escalado Controlado)\n",
      "2025-05-23 15:53:36,838 INFO     Output directory: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\n",
      "2025-05-23 15:53:36,838 INFO     =================================================\n",
      "\n",
      "=== Iniciando experimento: MEUS_MajClass_Scaling_scenario1 ===\n",
      "2025-05-23 15:53:38,137 INFO     Dataset cargado (sin escalar). Forma: (284807, 31). Clases: {0: 284315, 1: 492}\n",
      "2025-05-23 15:53:38,165 INFO     Usando MEUS (escalado interno para Mahalanobis, escalado de modelado controlado por escenario).\n",
      "2025-05-23 15:53:38,166 INFO     >> ESCENARIO 1: MEUS en TODO el dataset (sin escalar) -> Split -> Escalado Separado\n",
      "2025-05-23 15:53:38,586 INFO     MEUS Scen1: Resampled. Majority selected: 492, Minority: 492\n",
      "2025-05-23 15:53:38,597 INFO        MEUS Scen1: Tamaño después de resample (antes de split y escalar): (984, 30), Distribución y_res: {1: 492, 0: 492}\n",
      "2025-05-23 15:53:38,602 INFO        MEUS Scen1: Después de Split. X_train_raw: (787, 30), X_test_raw: (197, 30)\n",
      "2025-05-23 15:53:38,605 INFO        MEUS Scen1: Después de Escalado. X_train_final: (787, 30), X_test_final: (197, 30)\n",
      "2025-05-23 15:53:38,606 INFO        → Tamaños finales para modelado. Train: (787, 30), Test: (197, 30)\n",
      "2025-05-23 15:53:38,607 INFO        → Distribución y_train_final: {1: 394, 0: 393}\n",
      "2025-05-23 15:53:38,608 INFO        → Distribución y_test_final: {0: 99, 1: 98}\n",
      "> Hyperparameters will be saved to: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\hyperparameters.txt\n",
      "\n",
      "> Entrenando NN...\n",
      "2025-05-23 15:53:38,617 INFO     Entrenando NN...\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n",
      "2025-05-23 15:54:25,070 INFO       ✔ Scaler NN guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\MEUS_MajClass_Scaling_scenario1_nn_scaler.joblib\n",
      "2025-05-23 15:54:25,071 INFO       ✔ Modelo Keras NN guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\MEUS_MajClass_Scaling_scenario1_nn_keras_model.h5\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para NN\n",
      "\n",
      "> Entrenando LOGREG...\n",
      "2025-05-23 15:54:25,072 INFO     Entrenando LOGREG...\n",
      "Fitting 4 folds for each of 18 candidates, totalling 72 fits\n",
      "2025-05-23 15:54:33,180 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\MEUS_MajClass_Scaling_scenario1_logreg_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para LOGREG\n",
      "\n",
      "> Entrenando SVM...\n",
      "2025-05-23 15:54:33,181 INFO     Entrenando SVM...\n",
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "2025-05-23 15:54:36,753 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\MEUS_MajClass_Scaling_scenario1_svm_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para SVM\n",
      "\n",
      "> Entrenando RF...\n",
      "2025-05-23 15:54:36,754 INFO     Entrenando RF...\n",
      "Fitting 4 folds for each of 108 candidates, totalling 432 fits\n",
      "2025-05-23 15:55:45,551 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\MEUS_MajClass_Scaling_scenario1_rf_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para RF\n",
      "\n",
      "> Entrenando XGB...\n",
      "2025-05-23 15:55:45,552 INFO     Entrenando XGB...\n",
      "Fitting 4 folds for each of 432 candidates, totalling 1728 fits\n",
      "2025-05-23 15:58:38,317 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\MEUS_MajClass_Scaling_scenario1_xgb_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para XGB\n",
      "2025-05-23 15:58:38,318 INFO     Entrenamiento para MEUS_MajClass_Scaling_scenario1 completado en 299.70s\n",
      "\n",
      "> Reports will be saved to: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario1\\classification_reports.txt\n",
      "\n",
      "> Evaluando NN...\n",
      "2025-05-23 15:58:38,319 INFO     Evaluando NN...\n",
      "NN ROC-AUC: 0.9717\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9792    0.9495    0.9641        99\n",
      "   Fraude (1)     0.9505    0.9796    0.9648        98\n",
      "\n",
      "     accuracy                         0.9645       197\n",
      "    macro avg     0.9648    0.9645    0.9645       197\n",
      " weighted avg     0.9649    0.9645    0.9645       197\n",
      "\n",
      "  ✔ Reporte guardado para NN\n",
      "\n",
      "> Evaluando LOGREG...\n",
      "2025-05-23 15:58:38,423 INFO     Evaluando LOGREG...\n",
      "LOGREG ROC-AUC: 0.9865\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9375    0.9091    0.9231        99\n",
      "   Fraude (1)     0.9109    0.9388    0.9246        98\n",
      "\n",
      "     accuracy                         0.9239       197\n",
      "    macro avg     0.9242    0.9239    0.9239       197\n",
      " weighted avg     0.9243    0.9239    0.9238       197\n",
      "\n",
      "  ✔ Reporte guardado para LOGREG\n",
      "\n",
      "> Evaluando SVM...\n",
      "2025-05-23 15:58:38,429 INFO     Evaluando SVM...\n",
      "SVM ROC-AUC: 0.9941\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9694    0.9596    0.9645        99\n",
      "   Fraude (1)     0.9596    0.9694    0.9645        98\n",
      "\n",
      "     accuracy                         0.9645       197\n",
      "    macro avg     0.9645    0.9645    0.9645       197\n",
      " weighted avg     0.9645    0.9645    0.9645       197\n",
      "\n",
      "  ✔ Reporte guardado para SVM\n",
      "\n",
      "> Evaluando RF...\n",
      "2025-05-23 15:58:38,437 INFO     Evaluando RF...\n",
      "RF ROC-AUC: 0.9971\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9899    0.9899    0.9899        99\n",
      "   Fraude (1)     0.9898    0.9898    0.9898        98\n",
      "\n",
      "     accuracy                         0.9898       197\n",
      "    macro avg     0.9898    0.9898    0.9898       197\n",
      " weighted avg     0.9898    0.9898    0.9898       197\n",
      "\n",
      "  ✔ Reporte guardado para RF\n",
      "\n",
      "> Evaluando XGB...\n",
      "2025-05-23 15:58:38,471 INFO     Evaluando XGB...\n",
      "XGB ROC-AUC: 0.9989\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9899    0.9899    0.9899        99\n",
      "   Fraude (1)     0.9898    0.9898    0.9898        98\n",
      "\n",
      "     accuracy                         0.9898       197\n",
      "    macro avg     0.9898    0.9898    0.9898       197\n",
      " weighted avg     0.9898    0.9898    0.9898       197\n",
      "\n",
      "  ✔ Reporte guardado para XGB\n",
      "2025-05-23 15:58:38,483 INFO     === Fin experimento MEUS_MajClass_Scaling_scenario1 ===\n",
      "\n",
      "=== Fin experimento: MEUS_MajClass_Scaling_scenario1 ===\n",
      "\n",
      "2025-05-23 15:58:38,484 INFO     =================================================\n",
      "2025-05-23 15:58:38,485 INFO     === Iniciando experimento: MEUS_MajClass_Scaling_scenario2 ===\n",
      "2025-05-23 15:58:38,486 INFO     Técnica: MEUS (Mahalanobis Undersampling en Clase Mayoritaria con Escalado Controlado)\n",
      "2025-05-23 15:58:38,486 INFO     Output directory: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\n",
      "2025-05-23 15:58:38,487 INFO     =================================================\n",
      "\n",
      "=== Iniciando experimento: MEUS_MajClass_Scaling_scenario2 ===\n",
      "2025-05-23 15:58:39,732 INFO     Dataset cargado (sin escalar). Forma: (284807, 31). Clases: {0: 284315, 1: 492}\n",
      "2025-05-23 15:58:39,759 INFO     Usando MEUS (escalado interno para Mahalanobis, escalado de modelado controlado por escenario).\n",
      "2025-05-23 15:58:39,760 INFO     >> ESCENARIO 2: Split del dataset (sin escalar) -> Escalado Separado -> MEUS (solo en train escalado)\n",
      "2025-05-23 15:58:39,896 INFO        MEUS Scen2: Después de Split inicial. X_train_raw: (227845, 30), X_test_raw: (56962, 30)\n",
      "2025-05-23 15:58:39,942 INFO        MEUS Scen2: Después de Escalado. X_train_scaled_for_meus: (227845, 30), X_test_final: (56962, 30)\n",
      "2025-05-23 15:58:40,273 INFO     MEUS Scen2: Resampled train. Majority selected: 394, Minority: 394\n",
      "2025-05-23 15:58:40,282 INFO        MEUS Scen2: Después de MEUS en train. X_train_final: (788, 30), y_train_final dist: {1: 394, 0: 394}\n",
      "2025-05-23 15:58:40,283 INFO        → Tamaños finales para modelado. Train: (788, 30), Test: (56962, 30)\n",
      "2025-05-23 15:58:40,285 INFO        → Distribución y_train_final: {1: 394, 0: 394}\n",
      "2025-05-23 15:58:40,286 INFO        → Distribución y_test_final: {0: 56864, 1: 98}\n",
      "> Hyperparameters will be saved to: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\hyperparameters.txt\n",
      "\n",
      "> Entrenando NN...\n",
      "2025-05-23 15:58:40,301 INFO     Entrenando NN...\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n",
      "2025-05-23 15:59:30,115 INFO       ✔ Scaler NN guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\MEUS_MajClass_Scaling_scenario2_nn_scaler.joblib\n",
      "2025-05-23 15:59:30,116 INFO       ✔ Modelo Keras NN guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\MEUS_MajClass_Scaling_scenario2_nn_keras_model.h5\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para NN\n",
      "\n",
      "> Entrenando LOGREG...\n",
      "2025-05-23 15:59:30,117 INFO     Entrenando LOGREG...\n",
      "Fitting 4 folds for each of 18 candidates, totalling 72 fits\n",
      "2025-05-23 15:59:39,531 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\MEUS_MajClass_Scaling_scenario2_logreg_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para LOGREG\n",
      "\n",
      "> Entrenando SVM...\n",
      "2025-05-23 15:59:39,532 INFO     Entrenando SVM...\n",
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "2025-05-23 15:59:43,209 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\MEUS_MajClass_Scaling_scenario2_svm_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para SVM\n",
      "\n",
      "> Entrenando RF...\n",
      "2025-05-23 15:59:43,210 INFO     Entrenando RF...\n",
      "Fitting 4 folds for each of 108 candidates, totalling 432 fits\n",
      "2025-05-23 16:00:56,733 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\MEUS_MajClass_Scaling_scenario2_rf_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para RF\n",
      "\n",
      "> Entrenando XGB...\n",
      "2025-05-23 16:00:56,734 INFO     Entrenando XGB...\n",
      "Fitting 4 folds for each of 432 candidates, totalling 1728 fits\n",
      "2025-05-23 16:03:56,553 INFO       ✔ Modelo (pipeline) guardado: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\MEUS_MajClass_Scaling_scenario2_xgb_best_pipeline.joblib\n",
      "  ✔ Hiperparámetros (o estado de error) guardados para XGB\n",
      "2025-05-23 16:03:56,556 INFO     Entrenamiento para MEUS_MajClass_Scaling_scenario2 completado en 316.26s\n",
      "\n",
      "> Reports will be saved to: C:/Users/saave/Desktop/data_balance/Result_MEUS\\MEUS_MajClass_Scaling_scenario2\\classification_reports.txt\n",
      "\n",
      "> Evaluando NN...\n",
      "2025-05-23 16:03:56,557 INFO     Evaluando NN...\n",
      "NN ROC-AUC: 0.9395\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9996    0.1969    0.3290     56864\n",
      "   Fraude (1)     0.0021    0.9592    0.0041        98\n",
      "\n",
      "     accuracy                         0.1982     56962\n",
      "    macro avg     0.5008    0.5780    0.1665     56962\n",
      " weighted avg     0.9979    0.1982    0.3284     56962\n",
      "\n",
      "  ✔ Reporte guardado para NN\n",
      "\n",
      "> Evaluando LOGREG...\n",
      "2025-05-23 16:03:57,090 INFO     Evaluando LOGREG...\n",
      "LOGREG ROC-AUC: 0.7412\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9990    0.2243    0.3664     56864\n",
      "   Fraude (1)     0.0019    0.8673    0.0038        98\n",
      "\n",
      "     accuracy                         0.2254     56962\n",
      "    macro avg     0.5005    0.5458    0.1851     56962\n",
      " weighted avg     0.9973    0.2254    0.3657     56962\n",
      "\n",
      "  ✔ Reporte guardado para LOGREG\n",
      "\n",
      "> Evaluando SVM...\n",
      "2025-05-23 16:03:57,161 INFO     Evaluando SVM...\n",
      "SVM ROC-AUC: 0.6742\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9971    0.0367    0.0709     56864\n",
      "   Fraude (1)     0.0017    0.9388    0.0033        98\n",
      "\n",
      "     accuracy                         0.0383     56962\n",
      "    macro avg     0.4994    0.4878    0.0371     56962\n",
      " weighted avg     0.9954    0.0383    0.0707     56962\n",
      "\n",
      "  ✔ Reporte guardado para SVM\n",
      "\n",
      "> Evaluando RF...\n",
      "2025-05-23 16:03:57,580 INFO     Evaluando RF...\n",
      "RF ROC-AUC: 0.6955\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9995    0.0346    0.0668     56864\n",
      "   Fraude (1)     0.0018    0.9898    0.0035        98\n",
      "\n",
      "     accuracy                         0.0362     56962\n",
      "    macro avg     0.5006    0.5122    0.0352     56962\n",
      " weighted avg     0.9978    0.0362    0.0667     56962\n",
      "\n",
      "  ✔ Reporte guardado para RF\n",
      "\n",
      "> Evaluando XGB...\n",
      "2025-05-23 16:03:58,053 INFO     Evaluando XGB...\n",
      "XGB ROC-AUC: 0.6758\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Fraude (0)     0.9990    0.0521    0.0991     56864\n",
      "   Fraude (1)     0.0018    0.9694    0.0035        98\n",
      "\n",
      "     accuracy                         0.0537     56962\n",
      "    macro avg     0.5004    0.5108    0.0513     56962\n",
      " weighted avg     0.9973    0.0537    0.0989     56962\n",
      "\n",
      "  ✔ Reporte guardado para XGB\n",
      "2025-05-23 16:03:58,164 INFO     === Fin experimento MEUS_MajClass_Scaling_scenario2 ===\n",
      "\n",
      "=== Fin experimento: MEUS_MajClass_Scaling_scenario2 ===\n",
      "\n",
      "Todos los experimentos MEUS con escalado controlado han finalizado.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones auxiliares\n",
    "# -------------------------------------------------------------------\n",
    "def setup_logging(log_path):\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    root_logger = logging.getLogger()\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=LOG_FMT,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path, mode='w'),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def load_and_prepare(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.rename(columns={'Time':'Tiempo','Amount':'Cantidad','Class':'Clase'})\n",
    "    return df\n",
    "\n",
    "def build_nn_model(n_inputs, learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Dense(32, input_shape=(n_inputs,), activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones de resampling MEUS (Escalado INTERNO de todas las features de clase may. para Mahalanobis)\n",
    "# Estas funciones devuelven datos en su escala ORIGINAL (la que tenían al entrar)\n",
    "# -------------------------------------------------------------------\n",
    "def meus_resample_logic(data_features_orig: pd.DataFrame, # Características de la clase mayoritaria (sin escalar para modelado)\n",
    "                        n_minority_samples: int,\n",
    "                        operation_context: str # \"Scen1\" o \"Scen2\" para logging\n",
    "                        ) -> pd.Index: # Devuelve los índices de la clase mayoritaria a conservar\n",
    "    \"\"\"\n",
    "    Lógica central de MEUS para seleccionar muestras de la clase mayoritaria.\n",
    "    Escala INTERNAMENTE todas las características de 'data_features_orig' para el cálculo de Mahalanobis.\n",
    "    Devuelve los índices de 'data_features_orig' a conservar.\n",
    "    \"\"\"\n",
    "    if data_features_orig.empty or data_features_orig.shape[1] == 0:\n",
    "        logging.warning(f\"MEUS {operation_context}: La clase mayoritaria no tiene características. \"\n",
    "                        \"Devolviendo todos los índices disponibles de la clase mayoritaria (si n_minority lo permite).\")\n",
    "        return data_features_orig.head(n_minority_samples).index # o .index si n_minority_samples > len\n",
    "\n",
    "    # Escalado INTERNO de TODAS las características de la clase mayoritaria para Mahalanobis\n",
    "    scaler_internal = MinMaxScaler()\n",
    "    # data_features_orig ya es SOLO las features de la clase mayoritaria\n",
    "    X_maj_scaled_for_mahalanobis = pd.DataFrame(scaler_internal.fit_transform(data_features_orig),\n",
    "                                                columns=data_features_orig.columns,\n",
    "                                                index=data_features_orig.index)\n",
    "\n",
    "    mean_vector = X_maj_scaled_for_mahalanobis.mean().values\n",
    "    \n",
    "    if X_maj_scaled_for_mahalanobis.shape[0] <= X_maj_scaled_for_mahalanobis.shape[1]:\n",
    "        logging.warning(f\"MEUS {operation_context}: Covarianza para clase mayoritaria podría ser singular (muestras={X_maj_scaled_for_mahalanobis.shape[0]}, feats={X_maj_scaled_for_mahalanobis.shape[1]}). Usando ddof=0.\")\n",
    "        cov_matrix = np.cov(X_maj_scaled_for_mahalanobis.values, rowvar=False, ddof=0)\n",
    "    else:\n",
    "        cov_matrix = np.cov(X_maj_scaled_for_mahalanobis.values, rowvar=False)\n",
    "\n",
    "    try:\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    except np.linalg.LinAlgError:\n",
    "        logging.warning(f\"MEUS {operation_context}: Matriz de covarianza de clase mayoritaria singular, usando pseudo-inversa.\")\n",
    "        inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "    \n",
    "    diff_values_maj = X_maj_scaled_for_mahalanobis.values - mean_vector\n",
    "    left_maj = diff_values_maj.dot(inv_cov_matrix)\n",
    "    distances_sq_maj = np.einsum('ij,ij->i', left_maj, diff_values_maj)\n",
    "    distances_maj = np.sqrt(np.maximum(distances_sq_maj, 0))\n",
    "    \n",
    "    # Añadir distancias al DataFrame de características ORIGINALES de la clase mayoritaria (para usar sus índices)\n",
    "    data_features_with_dist = data_features_orig.copy() # Evitar SettingWithCopyWarning\n",
    "    data_features_with_dist['Mahalanobis_Distance_Internal'] = distances_maj\n",
    "    \n",
    "    if len(data_features_with_dist) > n_minority_samples:\n",
    "        selected_indices = data_features_with_dist.sort_values(\n",
    "            by='Mahalanobis_Distance_Internal', ascending=False\n",
    "        ).head(n_minority_samples).index\n",
    "    else:\n",
    "        selected_indices = data_features_with_dist.index\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "def meus_scenario1_apply(df_orig: pd.DataFrame, target_col_name='Clase', majority_class_label=0, minority_class_label=1) -> (pd.DataFrame, pd.Series):\n",
    "    \"\"\"\n",
    "    Aplica MEUS al dataset completo (df_orig, que NO está escalado para modelado).\n",
    "    Devuelve X_res, y_res en su escala original.\n",
    "    \"\"\"\n",
    "    df = df_orig.copy()\n",
    "    minority_samples = df[df[target_col_name] == minority_class_label]\n",
    "    majority_samples_df_orig = df[df[target_col_name] == majority_class_label]\n",
    "    \n",
    "    n_minority = len(minority_samples)\n",
    "    \n",
    "    if n_minority == 0:\n",
    "        logging.warning(\"MEUS Scen1: No hay muestras de la clase minoritaria. Devolviendo datos originales.\")\n",
    "        return df.drop(columns=[target_col_name]), df[target_col_name]\n",
    "    if len(majority_samples_df_orig) == 0:\n",
    "        logging.warning(\"MEUS Scen1: No hay muestras de la clase mayoritaria. Devolviendo datos originales.\")\n",
    "        return df.drop(columns=[target_col_name]), df[target_col_name]\n",
    "\n",
    "    X_maj_features_orig = majority_samples_df_orig.drop(columns=[target_col_name])\n",
    "    \n",
    "    # Fallback si no hay features en la clase mayoritaria (ej. solo columna target)\n",
    "    if X_maj_features_orig.empty:\n",
    "        logging.warning(\"MEUS Scen1: Clase mayoritaria sin features. Submuestreo aleatorio.\")\n",
    "        if len(majority_samples_df_orig) > n_minority:\n",
    "            selected_majority_indices = majority_samples_df_orig.sample(n=n_minority, random_state=42).index\n",
    "        else:\n",
    "            selected_majority_indices = majority_samples_df_orig.index\n",
    "    else:\n",
    "        selected_majority_indices = meus_resample_logic(X_maj_features_orig, n_minority, \"Scen1\")\n",
    "\n",
    "    minority_indices = minority_samples.index\n",
    "    balanced_indices = pd.Index(list(selected_majority_indices) + list(minority_indices))\n",
    "    \n",
    "    # Seleccionar del DataFrame ORIGINAL (df_orig)\n",
    "    df_resampled = df_orig.loc[balanced_indices].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    X_res = df_resampled.drop(columns=[target_col_name])\n",
    "    y_res = df_resampled[target_col_name]\n",
    "    \n",
    "    logging.info(f\"MEUS Scen1: Resampled. Majority selected: {len(selected_majority_indices)}, Minority: {len(minority_indices)}\")\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def meus_scenario2_apply(X_train_scaled_for_balance: pd.DataFrame, # X_train YA ESCALADO para modelado\n",
    "                         y_train_orig: pd.Series,\n",
    "                         majority_class_label=0, minority_class_label=1) -> (pd.DataFrame, pd.Series):\n",
    "    \"\"\"\n",
    "    Aplica MEUS al conjunto de entrenamiento (X_train_scaled_for_balance, y_train_orig).\n",
    "    X_train_scaled_for_balance ya está escalado para modelado.\n",
    "    \"\"\"\n",
    "    X_train_current = X_train_scaled_for_balance.copy() # Datos ya escalados para modelado\n",
    "    y_train = y_train_orig.copy()\n",
    "\n",
    "    df_train_temp = X_train_current.assign(Clase_temp_target=y_train)\n",
    "    minority_samples_train = df_train_temp[df_train_temp['Clase_temp_target'] == minority_class_label]\n",
    "    majority_samples_train_df = df_train_temp[df_train_temp['Clase_temp_target'] == majority_class_label]\n",
    "    \n",
    "    n_minority_train = len(minority_samples_train)\n",
    "\n",
    "    if n_minority_train == 0:\n",
    "        logging.warning(\"MEUS Scen2: No hay muestras minoritarias en train. Devolviendo train original (escalado).\")\n",
    "        return X_train_current, y_train\n",
    "    if len(majority_samples_train_df) == 0:\n",
    "        logging.warning(\"MEUS Scen2: No hay muestras mayoritarias en train. Devolviendo train original (escalado).\")\n",
    "        return X_train_current, y_train\n",
    "\n",
    "    # X_maj_features_train_scaled son las características de la clase mayoritaria, YA ESCALADAS para modelado\n",
    "    X_maj_features_train_scaled = majority_samples_train_df.drop(columns=['Clase_temp_target'])\n",
    "\n",
    "    # Fallback si no hay features en la clase mayoritaria\n",
    "    if X_maj_features_train_scaled.empty:\n",
    "        logging.warning(\"MEUS Scen2: Clase mayoritaria en train sin features. Submuestreo aleatorio.\")\n",
    "        if len(majority_samples_train_df) > n_minority_train:\n",
    "            selected_majority_indices = majority_samples_train_df.sample(n=n_minority_train, random_state=42).index\n",
    "        else:\n",
    "            selected_majority_indices = majority_samples_train_df.index\n",
    "    else:\n",
    "        # meus_resample_logic recibirá datos ya escalados. Su escalado interno MinMax no cambiará los datos.\n",
    "        selected_majority_indices = meus_resample_logic(X_maj_features_train_scaled, n_minority_train, \"Scen2\")\n",
    "\n",
    "    minority_indices_train = minority_samples_train.index\n",
    "    balanced_indices_train = pd.Index(list(selected_majority_indices) + list(minority_indices_train))\n",
    "    \n",
    "    # Seleccionar de df_train_temp (que contiene X_train_current y y_train)\n",
    "    df_resampled_train = df_train_temp.loc[balanced_indices_train].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    X_train_balanced_scaled = df_resampled_train.drop(columns=['Clase_temp_target'])\n",
    "    y_train_balanced = df_resampled_train['Clase_temp_target']\n",
    "\n",
    "    logging.info(f\"MEUS Scen2: Resampled train. Majority selected: {len(selected_majority_indices)}, Minority: {len(minority_indices_train)}\")\n",
    "    return X_train_balanced_scaled, y_train_balanced\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Función principal de escenario (SOLO PARA MEUS con nueva lógica de escalado)\n",
    "# -------------------------------------------------------------------\n",
    "def run_scenario_meus_controlled_scaling(df_original_unscaled, scenario):\n",
    "    target_col_name = 'Clase'\n",
    "    X_original_unscaled = df_original_unscaled.drop(target_col_name, axis=1)\n",
    "    y_original = df_original_unscaled[target_col_name]\n",
    "    \n",
    "    majority_class_label = 0 \n",
    "    minority_class_label = 1\n",
    "\n",
    "    X_train_final, X_test_final, y_train_final, y_test_final = [None]*4 # Inicializar\n",
    "\n",
    "    logging.info(f\"Usando MEUS (escalado interno para Mahalanobis, escalado de modelado controlado por escenario).\")\n",
    "\n",
    "    if scenario == 'scenario1':\n",
    "        # 1. Balance en TODO el dataset (df_original_unscaled)\n",
    "        #    meus_scenario1_apply devuelve X_res_unscaled, y_res\n",
    "        logging.info(f\">> ESCENARIO 1: MEUS en TODO el dataset (sin escalar) -> Split -> Escalado Separado\")\n",
    "        X_res_unscaled, y_res = meus_scenario1_apply(df_original_unscaled.copy(), \n",
    "                                                     target_col_name=target_col_name,\n",
    "                                                     majority_class_label=majority_class_label,\n",
    "                                                     minority_class_label=minority_class_label)\n",
    "        logging.info(f\"   MEUS Scen1: Tamaño después de resample (antes de split y escalar): {X_res_unscaled.shape}, Distribución y_res: {dict(y_res.value_counts())}\")\n",
    "\n",
    "        if X_res_unscaled.empty or y_res.empty:\n",
    "            logging.error(\"   MEUS Scen1: X_res_unscaled o y_res vacíos después del balanceo. Abortando escenario.\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "        # 2. Split del dataset balanceado (pero aún no escalado para modelado)\n",
    "        stratify_param_s1 = y_res if not y_res.empty and len(y_res.unique()) > 1 else None\n",
    "        X_train_raw, X_test_raw, y_train_final, y_test_final = train_test_split(\n",
    "            X_res_unscaled, y_res, test_size=0.2, stratify=stratify_param_s1, random_state=42\n",
    "        )\n",
    "        logging.info(f\"   MEUS Scen1: Después de Split. X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n",
    "\n",
    "        # 3. Escalado SEPARADO para modelado\n",
    "        if not X_train_raw.empty:\n",
    "            scaler_s1 = MinMaxScaler()\n",
    "            X_train_final = pd.DataFrame(scaler_s1.fit_transform(X_train_raw), columns=X_train_raw.columns, index=X_train_raw.index)\n",
    "            if not X_test_raw.empty:\n",
    "                X_test_final = pd.DataFrame(scaler_s1.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            else: X_test_final = pd.DataFrame() # Mantener vacío si raw estaba vacío\n",
    "            logging.info(f\"   MEUS Scen1: Después de Escalado. X_train_final: {X_train_final.shape}, X_test_final: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "        else: # Si X_train_raw es vacío\n",
    "            X_train_final = pd.DataFrame()\n",
    "            X_test_final = pd.DataFrame() # X_test_raw también sería vacío\n",
    "\n",
    "    else: # scenario2\n",
    "        # 1. Split del dataset ORIGINAL (df_original_unscaled)\n",
    "        logging.info(f\">> ESCENARIO 2: Split del dataset (sin escalar) -> Escalado Separado -> MEUS (solo en train escalado)\")\n",
    "        stratify_param_s2_initial = y_original if not y_original.empty and len(y_original.unique()) > 1 else None\n",
    "        X_train_raw, X_test_raw, y_train_orig, y_test_final = train_test_split(\n",
    "            X_original_unscaled, y_original, test_size=0.2, stratify=stratify_param_s2_initial, random_state=42\n",
    "        )\n",
    "        logging.info(f\"   MEUS Scen2: Después de Split inicial. X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n",
    "\n",
    "        # 2. Escalado \n",
    "        X_train_scaled_for_meus = pd.DataFrame()\n",
    "        X_test_final = pd.DataFrame()\n",
    "\n",
    "        if not X_train_raw.empty:\n",
    "            scaler_s2 = MinMaxScaler()\n",
    "            X_train_scaled_for_meus = pd.DataFrame(scaler_s2.fit_transform(X_train_raw), columns=X_train_raw.columns, index=X_train_raw.index)\n",
    "            if not X_test_raw.empty:\n",
    "                X_test_final = pd.DataFrame(scaler_s2.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            logging.info(f\"   MEUS Scen2: Después de Escalado. X_train_scaled_for_meus: {X_train_scaled_for_meus.shape}, X_test_final: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "        \n",
    "        # 3. MEUS solo en train (que ya está escalado para modelado)\n",
    "        #    meus_scenario2_apply recibe X_train escalado y devuelve X_train_balanced también escalado.\n",
    "        if not X_train_scaled_for_meus.empty and not y_train_orig.empty:\n",
    "            X_train_final, y_train_final = meus_scenario2_apply(X_train_scaled_for_meus, y_train_orig,\n",
    "                                                                majority_class_label=majority_class_label,\n",
    "                                                                minority_class_label=minority_class_label)\n",
    "            logging.info(f\"   MEUS Scen2: Después de MEUS en train. X_train_final: {X_train_final.shape}, y_train_final dist: {dict(y_train_final.value_counts() if not y_train_final.empty else {})}\")\n",
    "        else:\n",
    "            X_train_final = X_train_scaled_for_meus # Pasa el (posiblemente vacío) X_train escalado\n",
    "            y_train_final = y_train_orig # Pasa el y_train original\n",
    "\n",
    "    logging.info(f\"   → Tamaños finales para modelado. Train: {X_train_final.shape if not X_train_final.empty else '(empty)'}, Test: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "    if not (y_train_final is None or y_train_final.empty):\n",
    "        logging.info(f\"   → Distribución y_train_final: {dict(y_train_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_train_final está vacío o es None.\")\n",
    "    if not (y_test_final is None or y_test_final.empty):\n",
    "        logging.info(f\"   → Distribución y_test_final: {dict(y_test_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_test_final está vacío o es None.\")\n",
    "        \n",
    "    return X_train_final, X_test_final, y_train_final, y_test_final\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones de entrenamiento y evaluación \n",
    "# -------------------------------------------------------------------\n",
    "def train_and_save_models(X_train, y_train, exp_name, output_dir):\n",
    "    hyper_file = os.path.join(output_dir, 'hyperparameters.txt')\n",
    "    with open(hyper_file, 'w') as hf:\n",
    "        hf.write(f\"Hyperparameters for experiment {exp_name}\\n\")\n",
    "        hf.write(\"=\"*60 + \"\\n\\n\")\n",
    "    print(f\"> Hyperparameters will be saved to: {hyper_file}\")\n",
    "\n",
    "    if X_train.empty or X_train.shape[1] == 0: # Chequeo adicional\n",
    "        logging.error(f\"X_train está vacío o no tiene características ANTES de entrenar modelos para {exp_name}. Saltando entrenamiento.\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "    n_inputs = X_train.shape[1]\n",
    "\n",
    "    nn_wrapper = KerasClassifier(\n",
    "        build_fn=build_nn_model,\n",
    "        n_inputs=n_inputs,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    specs = {\n",
    "\n",
    "        'nn': (\n",
    "            nn_wrapper,\n",
    "            {\n",
    "                'clf__learning_rate': [0.0001], \n",
    "                'clf__dropout_rate':  [0.05, 0.1, 0.3], \n",
    "                'clf__batch_size':    [16, 32, 64], \n",
    "                'clf__epochs':        [100], \n",
    "            }\n",
    "        ),\n",
    "\n",
    "        'logreg': (\n",
    "            LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "            {'clf__penalty':['l1','l2'], 'clf__C':[0.001, 0.01, 0.7, 0.1, 0.2, 1, 10, 100, 1000], 'clf__solver':['liblinear']}\n",
    "        ),\n",
    "        'svm': (\n",
    "            SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "            {'clf__C': [0.5, 0.7, 0.9, 1, 1.5], 'clf__kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "        ),\n",
    "        'rf': (\n",
    "            RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "            {'clf__n_estimators':[50, 100, 200], 'clf__max_depth':[None, 10, 20, 30], 'clf__min_samples_split': [2, 5, 10], 'clf__min_samples_leaf': [1, 2, 4]}\n",
    "        ),\n",
    "        'xgb': (\n",
    "            XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "             {\n",
    "                'clf__n_estimators': [50, 100, 200],\n",
    "                'clf__max_depth': [3, 5, 7, 10],\n",
    "                'clf__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "                'clf__subsample': [0.7, 0.8, 0.9],\n",
    "                'clf__colsample_bytree': [0.7, 0.8, 1]\n",
    "            }\n",
    "        ),\n",
    "\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, (clf, param_grid) in specs.items():\n",
    "        print(f\"\\n> Entrenando {name.upper()}...\")\n",
    "        logging.info(f\"Entrenando {name.upper()}...\")\n",
    "        \n",
    "        # El pipeline ahora aplica el escalado a X_train_final (que ya debería estar escalado).\n",
    "        # MinMaxScaler en datos ya escalados [0,1] no los cambia, así que es seguro.\n",
    "        # Si X_train_final NO estuviera escalado por alguna razón, este scaler lo haría.\n",
    "        pipe = ImbPipeline([ \n",
    "            ('scaler', MinMaxScaler()), \n",
    "            ('clf', clf)\n",
    "        ])\n",
    "        \n",
    "        y_train_processed = y_train.astype(int) \n",
    "\n",
    "        fit_params = {}\n",
    "        n_cv_splits = 4 \n",
    "        if len(y_train_processed.unique()) > 1:\n",
    "            min_class_count = min(y_train_processed.value_counts())\n",
    "            n_cv_splits = min(4, min_class_count) \n",
    "            if min_class_count < 2:\n",
    "                 logging.warning(f\"Clase minoritaria en y_train para {name} tiene {min_class_count} muestras. CV no es posible. GridSearchCV podría fallar.\")\n",
    "                 n_cv_splits = 2 # Forzar a 2, pero esperar fallo o manejo por error_score\n",
    "        \n",
    "        if X_train.shape[0] < n_cv_splits: # Si hay menos muestras que folds\n",
    "            logging.warning(f\"No hay suficientes muestras en X_train ({X_train.shape[0]}) para CV con {n_cv_splits} splits en {name}. Saltando GridSearchCV.\")\n",
    "            best_params_str = \"CV skipped (pocas muestras)\"\n",
    "            best_score_str = \"N/A (CV skipped)\"\n",
    "            # Podrías entrenar un modelo por defecto aquí si quieres\n",
    "            best_models[name] = None # Marcar como no entrenado\n",
    "            with open(hyper_file, 'a') as hf: # Guardar estado\n",
    "                hf.write(f\"{name.upper()} best params: {best_params_str}\\n\")\n",
    "                hf.write(f\"{name.upper()} best CV ROC-AUC: {best_score_str}\\n\\n\")\n",
    "            print(f\"  ! Hiperparámetros (o estado de error) guardados para {name.upper()}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            pipe, param_grid, cv=n_cv_splits, scoring='roc_auc', \n",
    "            n_jobs=1, verbose=1, refit=True, error_score='raise' \n",
    "        )\n",
    "        \n",
    "        best_estimator_for_model = None\n",
    "        try:\n",
    "            grid.fit(X_train, y_train_processed, **fit_params)            \n",
    "            best_estimator_for_model = grid.best_estimator_\n",
    "            best_params_str = str(grid.best_params_)\n",
    "            best_score_str = f\"{grid.best_score_:.4f}\"\n",
    "\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"Error de ValueError (posiblemente CV) durante GridSearchCV para {name} en {exp_name}: {ve}\", exc_info=True)\n",
    "            print(f\"Error de ValueError durante GridSearchCV para {name}, saltando este modelo: {ve}\")\n",
    "            best_params_str = \"Error en CV\"\n",
    "            best_score_str = \"Error en CV\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error general durante GridSearchCV para {name} en {exp_name}: {e}\", exc_info=True)\n",
    "            print(f\"Error general durante GridSearchCV para {name}, saltando este modelo: {e}\")\n",
    "            best_params_str = \"Error general\"\n",
    "            best_score_str = \"Error general\"\n",
    "\n",
    "        if best_estimator_for_model:\n",
    "            if name == 'nn':\n",
    "                # ... (código de guardado NN igual)\n",
    "                scaler_nn = best_estimator_for_model.named_steps['scaler']\n",
    "                keras_model_nn = best_estimator_for_model.named_steps['clf'].model\n",
    "                scaler_path = os.path.join(output_dir, f\"{exp_name}_{name}_scaler.joblib\")\n",
    "                keras_model_path = os.path.join(output_dir, f\"{exp_name}_{name}_keras_model.h5\")\n",
    "                joblib.dump(scaler_nn, scaler_path)\n",
    "                keras_model_nn.save(keras_model_path)\n",
    "                logging.info(f\"  ✔ Scaler NN guardado: {scaler_path}\")\n",
    "                logging.info(f\"  ✔ Modelo Keras NN guardado: {keras_model_path}\")\n",
    "                best_models[name] = (scaler_path, keras_model_path)\n",
    "            else:\n",
    "                # ... (código de guardado otros modelos igual)\n",
    "                model_path = os.path.join(output_dir, f\"{exp_name}_{name}_best_pipeline.joblib\")\n",
    "                joblib.dump(best_estimator_for_model, model_path)\n",
    "                logging.info(f\"  ✔ Modelo (pipeline) guardado: {model_path}\")\n",
    "                best_models[name] = best_estimator_for_model\n",
    "        else:\n",
    "            best_models[name] = None # Marcar explícitamente que no se entrenó/guardó\n",
    "            logging.warning(f\"No se pudo obtener best_estimator para {name} debido a un error previo.\")\n",
    "\n",
    "        with open(hyper_file, 'a') as hf:\n",
    "            hf.write(f\"{name.upper()} best params: {best_params_str}\\n\")\n",
    "            hf.write(f\"{name.upper()} best CV ROC-AUC: {best_score_str}\\n\\n\")\n",
    "        print(f\"  ✔ Hiperparámetros (o estado de error) guardados para {name.upper()}\")\n",
    "        \n",
    "    return {k: v for k, v in best_models.items() if v is not None} # Devolver solo modelos exitosos\n",
    "\n",
    "\n",
    "def evaluate_and_save_reports(models, X_test, y_test, output_dir):\n",
    "   \n",
    "    report_file = os.path.join(output_dir, \"classification_reports.txt\")\n",
    "    with open(report_file, \"w\") as rf:\n",
    "        rf.write(f\"Classification reports for {os.path.basename(output_dir)}\\n\")\n",
    "        rf.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    print(f\"\\n> Reports will be saved to: {report_file}\")\n",
    "\n",
    "    if X_test.empty or y_test.empty: # X_test puede ser vacío si X_train_raw era vacío\n",
    "        logging.error(\"X_test o y_test están vacíos. No se pueden generar reportes.\")\n",
    "        print(\"X_test o y_test están vacíos. Saltando evaluación.\")\n",
    "        return\n",
    "\n",
    "    y_test_processed = y_test.astype(int)\n",
    "\n",
    "    for name, model_or_paths in models.items():\n",
    "        # El modelo ya viene con su scaler si no es NN, o las rutas para NN\n",
    "        print(f\"\\n> Evaluando {name.upper()}...\")\n",
    "        logging.info(f\"Evaluando {name.upper()}...\")\n",
    "        y_pred, y_prob = None, None \n",
    "\n",
    "        try:\n",
    "            if name == \"nn\":\n",
    "                # model_or_paths son (scaler_path, keras_model_path)\n",
    "                scaler_path, keras_model_path = model_or_paths\n",
    "                scaler_loaded = joblib.load(scaler_path)\n",
    "                keras_model = load_model(keras_model_path, compile=False)\n",
    "                X_test_for_eval = scaler_loaded.transform(X_test) \n",
    "                \n",
    "                y_prob_all_classes = keras_model.predict(X_test_for_eval)\n",
    "                y_prob = y_prob_all_classes[:, 1]\n",
    "                y_pred = np.argmax(y_prob_all_classes, axis=1)\n",
    "            else: \n",
    "                # model_or_paths es el pipeline completo (scaler + clf)\n",
    "                model_pipeline = model_or_paths\n",
    "                # El pipeline se encargará de escalar X_test (ya debería estar escalado, pero es seguro)\n",
    "                y_prob_all_classes = model_pipeline.predict_proba(X_test) \n",
    "                y_pred = model_pipeline.predict(X_test)\n",
    "                \n",
    "                if y_prob_all_classes.shape[1] == 2:\n",
    "                    y_prob = y_prob_all_classes[:, 1]\n",
    "                elif y_prob_all_classes.shape[1] == 1: \n",
    "                    clf_step = model_pipeline.named_steps.get('clf')\n",
    "                    if clf_step and hasattr(clf_step, 'classes_') and len(clf_step.classes_) == 1:\n",
    "                        if clf_step.classes_[0] == 1: y_prob = y_prob_all_classes[:, 0]\n",
    "                        else: y_prob = 1.0 - y_prob_all_classes[:, 0]\n",
    "                    else: y_prob = np.zeros(len(y_test_processed)) \n",
    "                else: y_prob = y_prob_all_classes[:, -1]\n",
    "\n",
    "            auc = float('nan')\n",
    "            report_str = \"N/A\"\n",
    "\n",
    "            if y_pred is not None and y_prob is not None:\n",
    "                if len(np.unique(y_test_processed)) < 2 :\n",
    "                    logging.warning(f\"Solo una clase presente en y_test para {name}. ROC-AUC no es calculable.\")\n",
    "                elif len(y_prob) == 0 :\n",
    "                    logging.warning(f\"y_prob está vacío para {name}. ROC-AUC no es calculable.\")\n",
    "                else:\n",
    "                    try:\n",
    "                        auc = roc_auc_score(y_test_processed, y_prob)\n",
    "                        if len(np.unique(y_pred)) < 2 and len(np.unique(y_test_processed)) >=2 :\n",
    "                            logging.warning(f\"Todas las predicciones son de una sola clase para {name}, pero y_test tiene variabilidad. ROC-AUC: {auc:.4f}\")\n",
    "                    except ValueError as e_auc:\n",
    "                        logging.error(f\"Error al calcular ROC-AUC para {name}: {e_auc}. y_test unique: {np.unique(y_test_processed)}, y_prob unique (first 5): {np.unique(y_prob[:5]) if len(y_prob)>0 else 'empty'}\")\n",
    "                \n",
    "                report_str = classification_report(\n",
    "                    y_test_processed, y_pred,\n",
    "                    target_names=[\"No Fraude (0)\", \"Fraude (1)\"],\n",
    "                    digits=4, zero_division=0\n",
    "                )\n",
    "            else:\n",
    "                logging.error(f\"Predicciones (y_pred o y_prob) no generadas para {name}.\")\n",
    "\n",
    "            print(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n{report_str}\")\n",
    "            with open(report_file, \"a\") as rf:\n",
    "                rf.write(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n{report_str}\\n{'-'*60}\\n\")\n",
    "            print(f\"  ✔ Reporte guardado para {name.upper()}\")\n",
    "        \n",
    "        except Exception as e_eval:\n",
    "            logging.error(f\"Error durante la evaluación de {name} en {output_dir}: {e_eval}\", exc_info=True)\n",
    "            print(f\"Error durante la evaluación de {name}: {e_eval}\")\n",
    "            with open(report_file, \"a\") as rf:\n",
    "                rf.write(f\"{name.upper()} - ERROR EN EVALUACIÓN: {e_eval}\\n{'-'*60}\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Bloque principal\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    CSV_PATH    = \"C:/Users/saave/Desktop/Master_Thesis/Credit_card_data/creditcard.csv\" \n",
    "    BASE_OUTPUT = \"C:/Users/saave/Desktop/data_balance/Result_MEUS\"\n",
    "    \n",
    "    for scenario in [\"scenario1\", \"scenario2\"]:\n",
    "        exp_name   = f\"MEUS_MajClass_Scaling_{scenario}\" \n",
    "        output_dir = os.path.join(BASE_OUTPUT, exp_name)\n",
    "        log_file   = os.path.join(output_dir, f\"run_{exp_name}.log\")\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        setup_logging(log_file)\n",
    "        \n",
    "        logging.info(f\"=================================================\")\n",
    "        logging.info(f\"=== Iniciando experimento: {exp_name} ===\")\n",
    "        logging.info(f\"Técnica: MEUS (Mahalanobis Undersampling en Clase Mayoritaria con Escalado Controlado)\")\n",
    "        logging.info(f\"Output directory: {output_dir}\")\n",
    "        logging.info(f\"=================================================\")\n",
    "        print(f\"\\n=== Iniciando experimento: {exp_name} ===\")\n",
    "\n",
    "        df_original = load_and_prepare(CSV_PATH) # Carga datos SIN escalar\n",
    "        logging.info(f\"Dataset cargado (sin escalar). Forma: {df_original.shape}. Clases: {dict(df_original['Clase'].value_counts())}\")\n",
    "\n",
    "        if df_original.shape[0] < 10: \n",
    "            logging.error(f\"Dataset con muy pocas filas ({df_original.shape[0]}). Abortando {exp_name}.\")\n",
    "            continue\n",
    "        class_counts = df_original['Clase'].value_counts()\n",
    "        if len(class_counts) < 2 or class_counts.get(0,0) == 0 or class_counts.get(1,0) == 0:\n",
    "            logging.error(f\"Dataset no tiene ambas clases o una clase está vacía: {class_counts.to_dict()}. Abortando {exp_name}.\")\n",
    "            continue\n",
    "\n",
    "        X_train, X_test, y_train, y_test = run_scenario_meus_controlled_scaling(df_original.copy(), scenario)\n",
    "\n",
    "        if X_train.empty or y_train.empty: # X_train podría ser vacío si X_res_unscaled es vacío\n",
    "            logging.error(f\"X_train o y_train vacíos para {exp_name} DESPUÉS de run_scenario. Saltando entrenamiento.\")\n",
    "            continue\n",
    "        if len(y_train.value_counts()) < 1: \n",
    "            logging.warning(f\"y_train no tiene muestras para {exp_name}. El entrenamiento podría fallar.\")\n",
    "        elif len(y_train.value_counts()) < 2:\n",
    "             logging.warning(f\"y_train tiene solo una clase para {exp_name}. GridSearchCV se adaptará o podría fallar.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        trained_models = train_and_save_models(X_train, y_train, exp_name, output_dir)\n",
    "        logging.info(f\"Entrenamiento para {exp_name} completado en {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        if not trained_models:\n",
    "            logging.warning(f\"No se entrenaron modelos exitosamente para {exp_name}. Saltando evaluación.\")\n",
    "        else:\n",
    "            # Asegurarse de que X_test no sea None o vacío antes de evaluar\n",
    "            if X_test is not None and not X_test.empty and y_test is not None and not y_test.empty:\n",
    "                 evaluate_and_save_reports(trained_models, X_test, y_test, output_dir)\n",
    "            else:\n",
    "                logging.warning(f\"X_test o y_test están vacíos para {exp_name}. Saltando evaluación.\")\n",
    "        \n",
    "        logging.info(f\"=== Fin experimento {exp_name} ===\\n\")\n",
    "        print(f\"=== Fin experimento: {exp_name} ===\\n\")\n",
    "\n",
    "    print(\"Todos los experimentos MEUS con escalado controlado han finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
