{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff84da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-23 12:24:33,840 INFO     TensorFlow inter-op parallelism threads set to 1.\n",
      "2025-05-23 12:24:33,841 INFO     TensorFlow intra-op parallelism threads set to 1.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import joblib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf # Importado para tf.random.set_seed\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    # StratifiedKFold, # No se usa directamente, GridSearchCV lo maneja\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "# Ignorar advertencias\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- CONFIGURACIÓN DE HILOS DE TENSORFLOW ---\n",
    "\n",
    "try:\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    logging.info(\"TensorFlow inter-op parallelism threads set to 1.\")\n",
    "    logging.info(\"TensorFlow intra-op parallelism threads set to 1.\")\n",
    "except RuntimeError as e:\n",
    "    # Esto puede ocurrir si los hilos ya fueron configurados (ej. en un entorno interactivo como Jupyter)\n",
    "    logging.warning(f\"Could not set TensorFlow threading: {e}. This might be okay if already configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-23 12:24:37,422 INFO     =================================================\n",
      "2025-05-23 12:24:37,423 INFO     === Iniciando experimento: NearMiss_Scaling_scenario1 ===\n",
      "2025-05-23 12:24:37,423 INFO     Técnica: NearMiss con Escalado Refinado (Idea para Scen1)\n",
      "2025-05-23 12:24:37,424 INFO     Output directory: C:/Users/saave/Desktop/data_balance/Result_NearMiss\\NearMiss_Scaling_scenario1\n",
      "2025-05-23 12:24:37,424 INFO     =================================================\n",
      "\n",
      "=== Iniciando: NearMiss_Scaling_scenario1 ===\n",
      "2025-05-23 12:24:38,726 INFO     Dataset cargado (sin escalar). Forma: (284807, 31). Clases: {0: 284315, 1: 492}\n",
      "2025-05-23 12:24:38,754 INFO     Usando NearMiss(version=1, n_neighbors=3) con escalado refinado.\n",
      "2025-05-23 12:24:38,755 INFO     >> ESCENARIO 1: Escalado Global TEMPORAL -> NearMiss -> Selección de ÍNDICES del original -> Split -> Escalado Separado LIMPIO para modelado\n",
      "2025-05-23 12:24:38,805 INFO        NearMiss Scen1: Datos escalados temporalmente para NearMiss. Forma: (284807, 30)\n",
      "2025-05-23 12:24:40,556 INFO        NearMiss Scen1: Datos seleccionados del original (sin escalar) usando índices de NearMiss. Forma X_res_unscaled: (984, 30), Distribución y_res: {0: 492, 1: 492}\n",
      "2025-05-23 12:24:40,559 INFO        NearMiss Scen1: Después de Split. X_train_raw: (787, 30), X_test_raw: (197, 30)\n",
      "2025-05-23 12:24:40,562 INFO        NearMiss Scen1: Después de Escalado LIMPIO para modelado. X_train_final: (787, 30), X_test_final: (197, 30)\n",
      "2025-05-23 12:24:40,562 INFO        → Tamaños finales para modelado. Train: (787, 30), Test: (197, 30)\n",
      "2025-05-23 12:24:40,563 INFO        → Distribución y_train_final: {1: 394, 0: 393}\n",
      "2025-05-23 12:24:40,563 INFO        → Distribución y_test_final: {0: 99, 1: 98}\n",
      "> Hyperparameters file: C:/Users/saave/Desktop/data_balance/Result_NearMiss\\NearMiss_Scaling_scenario1\\hyperparameters.txt\n",
      "\n",
      "> Entrenando NN...\n",
      "2025-05-23 12:24:40,572 INFO     Entrenando NN...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "\n",
    "# ... (Configuración global, create_nn_model, setup_logging, load_and_prepare sin cambios) ...\n",
    "# -------------------------------------------------------------------\n",
    "# Configuración global\n",
    "# -------------------------------------------------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "# tf.random.set_seed(42) \n",
    "LOG_FMT = \"%(asctime)s %(levelname)-8s %(message)s\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Constructor de la red neuronal (picklable)\n",
    "# -------------------------------------------------------------------\n",
    "def create_nn_model(n_inputs, learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Dense(32, input_shape=(n_inputs,), activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones auxiliares\n",
    "# -------------------------------------------------------------------\n",
    "def setup_logging(log_path):\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    root_logger = logging.getLogger()\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=LOG_FMT,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path, mode='w'),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def load_and_prepare(csv_path): # YA NO ESCALA INTERNAMENTE\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.rename(columns={'Time':'Tiempo','Amount':'Cantidad','Class':'Clase'})\n",
    "    return df\n",
    "# -------------------------------------------------------------------\n",
    "# Función principal de escenario (PARA NEARMISS con escalado controlado y tu idea para Scen1)\n",
    "# -------------------------------------------------------------------\n",
    "def run_scenario_nearmiss_refined_scaling(df_original_unscaled, scenario):\n",
    "    target_col_name = 'Clase'\n",
    "    # Asegurarse de que X_original_unscaled es un DataFrame y y_original es una Serie\n",
    "    if not isinstance(df_original_unscaled, pd.DataFrame):\n",
    "        raise TypeError(\"df_original_unscaled debe ser un DataFrame de Pandas.\")\n",
    "    \n",
    "    X_original_unscaled = df_original_unscaled.drop(target_col_name, axis=1, errors='ignore')\n",
    "    if target_col_name not in df_original_unscaled.columns:\n",
    "        raise ValueError(f\"La columna objetivo '{target_col_name}' no se encuentra en df_original_unscaled.\")\n",
    "    y_original = df_original_unscaled[target_col_name]\n",
    "\n",
    "\n",
    "    # Configuración de NearMiss\n",
    "    nearmiss_sampler = NearMiss(sampling_strategy='majority', version=1, n_neighbors=3)\n",
    "    logging.info(f\"Usando NearMiss(version={nearmiss_sampler.version}, n_neighbors={nearmiss_sampler.n_neighbors}) con escalado refinado.\")\n",
    "\n",
    "    X_train_final, X_test_final, y_train_final, y_test_final = [pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')] # Inicializar como vacíos\n",
    "\n",
    "    if scenario == 'scenario1':\n",
    "        # Escenario 1: Fuga de Datos SOLO en la Selección de Muestras por NearMiss\n",
    "        # 1. Cargar Datos (hecho) -> X_original_unscaled, y_original\n",
    "        logging.info(f\">> ESCENARIO 1: Escalado Global TEMPORAL -> NearMiss -> Selección de ÍNDICES del original -> Split -> Escalado Separado LIMPIO para modelado\")\n",
    "\n",
    "        # 2. Crear Copia para NearMiss y Escalarla Globalmente (Temporal)\n",
    "        if X_original_unscaled.empty:\n",
    "            logging.error(\"   NearMiss Scen1: X_original_unscaled está vacío. Abortando escenario.\")\n",
    "            return X_train_final, X_test_final, y_train_final, y_test_final\n",
    "            \n",
    "        scaler_temporal_s1 = MinMaxScaler()\n",
    "        X_temp_scaled_for_nearmiss = pd.DataFrame(\n",
    "            scaler_temporal_s1.fit_transform(X_original_unscaled), # .copy() implícito si es necesario\n",
    "            columns=X_original_unscaled.columns,\n",
    "            index=X_original_unscaled.index # Mantener índice original es CLAVE\n",
    "        )\n",
    "        logging.info(f\"   NearMiss Scen1: Datos escalados temporalmente para NearMiss. Forma: {X_temp_scaled_for_nearmiss.shape}\")\n",
    "\n",
    "        # 3. Balanceo con NearMiss (Sobre Datos Globalmente Escalados Temporalmente)\n",
    "        #    para obtener los ÍNDICES de las muestras seleccionadas.\n",
    "        X_res_unscaled = pd.DataFrame() # Inicializar como DataFrames vacíos\n",
    "        y_res = pd.Series(dtype='float64')\n",
    "\n",
    "        try:\n",
    "            # Comprobación de suficientes muestras para NearMiss\n",
    "            # NearMiss puede necesitar al menos n_neighbors muestras en la clase minoritaria (implícitamente) y en la mayoría.\n",
    "            # y_original.value_counts() nos da las cuentas de cada clase.\n",
    "            class_counts = y_original.value_counts()\n",
    "            minority_class_count = class_counts.min() if not class_counts.empty else 0\n",
    "  \n",
    "            can_run_nearmiss = True\n",
    "            if X_temp_scaled_for_nearmiss.empty or y_original.empty:\n",
    "                can_run_nearmiss = False\n",
    "            elif nearmiss_sampler.version in [1, 2]: # Selecciona de la clase mayoritaria\n",
    "                 if class_counts.get(0, 0) < nearmiss_sampler.n_neighbors : # Asumiendo 0 es mayoría\n",
    "                     can_run_nearmiss = False\n",
    "                     logging.warning(f\"   NearMiss Scen1: No hay suficientes muestras en la clase MAYORITARIA ({class_counts.get(0,0)}) para NearMiss v{nearmiss_sampler.version} con n_neighbors={nearmiss_sampler.n_neighbors}.\")\n",
    "            elif nearmiss_sampler.version == 3: # Selecciona de la clase minoritaria\n",
    "                 if minority_class_count < nearmiss_sampler.n_neighbors:\n",
    "                     can_run_nearmiss = False\n",
    "                     logging.warning(f\"   NearMiss Scen1: No hay suficientes muestras en la clase MINORITARIA ({minority_class_count}) para NearMiss v3 con n_neighbors={nearmiss_sampler.n_neighbors}.\")\n",
    "\n",
    "\n",
    "            if not can_run_nearmiss:\n",
    "                logging.warning(\"   NearMiss Scen1: No hay suficientes datos/clases para NearMiss. Usando datos originales sin balanceo.\")\n",
    "                X_res_unscaled = X_original_unscaled.copy()\n",
    "                y_res = y_original.copy()\n",
    "            else:\n",
    "                # Fitear NearMiss\n",
    "                nearmiss_sampler.fit_resample(X_temp_scaled_for_nearmiss, y_original)\n",
    "                # Obtener los índices de las muestras seleccionadas\n",
    "                selected_indices = nearmiss_sampler.sample_indices_\n",
    "                \n",
    "                # 4. Seleccionar Muestras del DataFrame Original NO ESCALADO usando los Índices\n",
    "                X_res_unscaled = X_original_unscaled.iloc[selected_indices].reset_index(drop=True)\n",
    "                y_res = y_original.iloc[selected_indices].reset_index(drop=True)\n",
    "                logging.info(f\"   NearMiss Scen1: Datos seleccionados del original (sin escalar) usando índices de NearMiss. \"\n",
    "                             f\"Forma X_res_unscaled: {X_res_unscaled.shape}, Distribución y_res: {dict(y_res.value_counts())}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"   NearMiss Scen1: Error durante NearMiss: {e}. Usando datos originales sin balanceo.\")\n",
    "            X_res_unscaled = X_original_unscaled.copy() # Fallback\n",
    "            y_res = y_original.copy()\n",
    "\n",
    "        if X_res_unscaled.empty or y_res.empty:\n",
    "            logging.error(\"   NearMiss Scen1: X_res_unscaled o y_res vacíos después del intento de balanceo. Abortando escenario.\")\n",
    "            return X_train_final, X_test_final, y_train_final, y_test_final # Devuelve vacíos\n",
    "\n",
    "        # 5. Split (División del Conjunto Balanceado NO ESCALADO)\n",
    "        stratify_param_s1 = y_res if not y_res.empty and len(y_res.unique()) > 1 else None\n",
    "        X_train_raw, X_test_raw, y_train_final, y_test_final = train_test_split(\n",
    "            X_res_unscaled, y_res, test_size=0.2, stratify=stratify_param_s1, random_state=42, shuffle=True\n",
    "        )\n",
    "        logging.info(f\"   NearMiss Scen1: Después de Split. X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n",
    "\n",
    "        # 6. Escalado Correcto para Modelado (SIN FUGA entre train/test de modelado)\n",
    "        if not X_train_raw.empty:\n",
    "            scaler_s1_modelado = MinMaxScaler()\n",
    "            X_train_final = pd.DataFrame(scaler_s1_modelado.fit_transform(X_train_raw), columns=X_train_raw.columns, index=X_train_raw.index)\n",
    "            if not X_test_raw.empty:\n",
    "                X_test_final = pd.DataFrame(scaler_s1_modelado.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            logging.info(f\"   NearMiss Scen1: Después de Escalado LIMPIO para modelado. X_train_final: {X_train_final.shape}, X_test_final: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "        # Si X_train_raw es vacío, X_train_final y X_test_final permanecen vacíos\n",
    "\n",
    "    else: # scenario2 (Sin Fuga de Datos en Escalado ni Balanceo) - Lógica como antes\n",
    "        # 1. Split del dataset ORIGINAL (df_original_unscaled)\n",
    "        logging.info(f\">> ESCENARIO 2: Split del dataset (sin escalar) -> Escalado Separado LIMPIO -> NearMiss (solo en train escalado)\")\n",
    "        stratify_param_s2_initial = y_original if not y_original.empty and len(y_original.unique()) > 1 else None\n",
    "        X_train_raw, X_test_raw, y_train_orig, y_test_final_orig = train_test_split(\n",
    "            X_original_unscaled, y_original, test_size=0.2, stratify=stratify_param_s2_initial, random_state=42, shuffle=True\n",
    "        )\n",
    "        logging.info(f\"   NearMiss Scen2: Después de Split inicial. X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n",
    "\n",
    "        # 2. Escalado SEPARADO y LIMPIO para modelado (y para NearMiss en train)\n",
    "        X_train_scaled_for_nearmiss = pd.DataFrame() # Inicializar\n",
    "        # X_test_final ya está inicializado como vacío arriba\n",
    "        y_test_final = y_test_final_orig # y_test no cambia por el escalado de X\n",
    "\n",
    "        if not X_train_raw.empty:\n",
    "            scaler_s2_modelado = MinMaxScaler()\n",
    "            X_train_scaled_for_nearmiss = pd.DataFrame(scaler_s2_modelado.fit_transform(X_train_raw), columns=X_train_raw.columns, index=X_train_raw.index)\n",
    "            if not X_test_raw.empty:\n",
    "                X_test_final = pd.DataFrame(scaler_s2_modelado.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            logging.info(f\"   NearMiss Scen2: Después de Escalado LIMPIO. X_train_scaled_for_nearmiss: {X_train_scaled_for_nearmiss.shape}, X_test_final: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "        \n",
    "        # 3. NearMiss solo en train (que ya está escalado para modelado)\n",
    "        y_train_final = y_train_orig # Inicializar y_train_final\n",
    "        X_train_final = X_train_scaled_for_nearmiss.copy() # Inicializar X_train_final\n",
    "\n",
    "        if not X_train_scaled_for_nearmiss.empty and not y_train_orig.empty:\n",
    "            try:\n",
    "                # Comprobación de suficientes muestras para NearMiss en train set\n",
    "                can_run_nearmiss_train = True\n",
    "                class_counts_train = y_train_orig.value_counts()\n",
    "                minority_class_count_train = class_counts_train.min() if not class_counts_train.empty else 0\n",
    "\n",
    "                if nearmiss_sampler.version in [1, 2]:\n",
    "                     if class_counts_train.get(0,0) < nearmiss_sampler.n_neighbors: # Asume 0 es mayoría\n",
    "                         can_run_nearmiss_train = False\n",
    "                         logging.warning(f\"   NearMiss Scen2: No hay suficientes muestras en la clase MAYORITARIA del train ({class_counts_train.get(0,0)}) para NearMiss v{nearmiss_sampler.version} con n_neighbors={nearmiss_sampler.n_neighbors}.\")\n",
    "                elif nearmiss_sampler.version == 3:\n",
    "                     if minority_class_count_train < nearmiss_sampler.n_neighbors:\n",
    "                         can_run_nearmiss_train = False\n",
    "                         logging.warning(f\"   NearMiss Scen2: No hay suficientes muestras en la clase MINORITARIA del train ({minority_class_count_train}) para NearMiss v3 con n_neighbors={nearmiss_sampler.n_neighbors}.\")\n",
    "\n",
    "                if not can_run_nearmiss_train:\n",
    "                     logging.warning(f\"   NearMiss Scen2: No hay suficientes datos/clases en train para NearMiss. Usando train escalado sin balanceo.\")\n",
    "                     # X_train_final y y_train_final ya están seteados a los datos pre-balanceo\n",
    "                else:\n",
    "                    # NearMiss opera sobre X_train_scaled_for_nearmiss\n",
    "                    # Devuelve arrays numpy, hay que reconvertir\n",
    "                    X_res_np_train, y_res_np_train = nearmiss_sampler.fit_resample(X_train_scaled_for_nearmiss.to_numpy(), y_train_orig.to_numpy())\n",
    "                    X_train_final = pd.DataFrame(X_res_np_train, columns=X_train_scaled_for_nearmiss.columns)\n",
    "                    y_train_final = pd.Series(y_res_np_train, name=y_train_orig.name)\n",
    "                logging.info(f\"   NearMiss Scen2: Después de NearMiss en train. X_train_final (escalado): {X_train_final.shape}, y_train_final dist: {dict(y_train_final.value_counts() if not y_train_final.empty else {})}\")\n",
    "            except ValueError as e:\n",
    "                logging.error(f\"   NearMiss Scen2: Error durante NearMiss.fit_resample en train: {e}. Usando train escalado sin balanceo.\")\n",
    "                # X_train_final y y_train_final ya están seteados a los datos pre-balanceo\n",
    "        # Si X_train_scaled_for_nearmiss o y_train_orig son vacíos, X_train_final y y_train_final permanecen con sus valores iniciales.\n",
    "\n",
    "    logging.info(f\"   → Tamaños finales para modelado. Train: {X_train_final.shape if not X_train_final.empty else '(empty)'}, Test: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "    if not (y_train_final is None or y_train_final.empty):\n",
    "        logging.info(f\"   → Distribución y_train_final: {dict(y_train_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_train_final está vacío o es None.\")\n",
    "    if not (y_test_final is None or y_test_final.empty):\n",
    "        logging.info(f\"   → Distribución y_test_final: {dict(y_test_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_test_final está vacío o es None.\")\n",
    "        \n",
    "    return X_train_final, X_test_final, y_train_final, y_test_final\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones de entrenamiento y evaluación (Prácticamente sin cambios)\n",
    "# -------------------------------------------------------------------\n",
    "def train_and_save_models(X_train, y_train, exp_name, output_dir):\n",
    "    hyper_file = os.path.join(output_dir, 'hyperparameters.txt')\n",
    "    with open(hyper_file, 'w') as hf:\n",
    "        hf.write(f\"Hyperparameters for {exp_name}\\n\")\n",
    "        hf.write(\"=\"*60 + \"\\n\\n\")\n",
    "    print(f\"> Hyperparameters file: {hyper_file}\")\n",
    "\n",
    "    if X_train.empty or X_train.shape[1] == 0:\n",
    "        logging.error(f\"X_train está vacío o no tiene características ANTES de entrenar modelos para {exp_name}. Saltando entrenamiento.\")\n",
    "        return {}\n",
    "\n",
    "    n_inputs = X_train.shape[1]\n",
    "    nn_wrapper = KerasClassifier(\n",
    "        build_fn=create_nn_model, \n",
    "        n_inputs=n_inputs,\n",
    "        verbose=0, \n",
    "    )\n",
    "\n",
    "    specs = {\n",
    "        'nn': (\n",
    "            nn_wrapper,\n",
    "            {\n",
    "                'clf__learning_rate':    [0.0001], \n",
    "                'clf__dropout_rate':     [0.3, 0.5],\n",
    "                'clf__batch_size':       [16,32, 64],\n",
    "                'clf__epochs':           [100],\n",
    "                'clf__validation_split': [0.1] \n",
    "            }\n",
    "        ),\n",
    "\n",
    "        'logreg': (\n",
    "            LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'), \n",
    "            {'clf__penalty': ['l1','l2'], 'clf__C': [0.001, 0.01, 0.7, 0.1, 0.2, 1, 10, 100, 1000], 'clf__solver': ['liblinear']}\n",
    "        ),\n",
    "        'svm': (\n",
    "            SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "            {'clf__C': [0.5, 0.7, 0.9, 1, 1.5], 'clf__kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "        ),\n",
    "        'rf': (\n",
    "            RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "            {'clf__n_estimators': [50, 100, 200], 'clf__max_depth': [None, 10, 20, 30], 'clf__min_samples_split': [2, 5, 10], 'clf__min_samples_leaf': [1, 2, 4]}\n",
    "        ),\n",
    "        'xgb': (\n",
    "            XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "            {'clf__n_estimators': [50, 100, 200], 'clf__max_depth': [3, 5, 7, 10], 'clf__learning_rate': [0.01, 0.1, 0.2, 0.3], 'clf__subsample': [0.7, 0.8, 0.9], 'clf__colsample_bytree': [0.7, 0.8, 1]}\n",
    "        )\n",
    "\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, (clf, param_grid) in specs.items():\n",
    "        print(f\"\\n> Entrenando {name.upper()}...\")\n",
    "        logging.info(f\"Entrenando {name.upper()}...\")\n",
    "        \n",
    "        pipe = ImbPipeline([ \n",
    "            ('scaler', MinMaxScaler()), \n",
    "            ('clf', clf)\n",
    "        ])\n",
    "        \n",
    "        y_train_processed = y_train.astype(int) \n",
    "\n",
    "        fit_params_grid = {}\n",
    "        if name == 'nn':\n",
    "            fit_params_grid['clf__callbacks'] = [\n",
    "                ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.2, min_lr=1e-6, verbose=0),\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0) \n",
    "            ]\n",
    "\n",
    "        n_cv_splits = 5 \n",
    "        min_samples_for_cv = n_cv_splits \n",
    "        \n",
    "        valid_cv = True\n",
    "        if len(y_train_processed.unique()) > 1:\n",
    "            min_class_count = min(y_train_processed.value_counts())\n",
    "            if min_class_count < n_cv_splits:\n",
    "                logging.warning(f\"Clase minoritaria en y_train para {name} tiene {min_class_count} muestras, menos que n_splits={n_cv_splits}. Intentando con n_splits={min_class_count if min_class_count >=2 else 2}.\")\n",
    "                n_cv_splits = max(2, min_class_count) # Necesita al menos 2, o el conteo de la clase si es >=2\n",
    "                min_samples_for_cv = n_cv_splits\n",
    "        else: # Solo una clase\n",
    "            logging.warning(f\"Solo una clase en y_train para {name}. CV no es posible.\")\n",
    "            valid_cv = False\n",
    "        \n",
    "        if X_train.shape[0] < min_samples_for_cv:\n",
    "            logging.warning(f\"No hay suficientes muestras en X_train ({X_train.shape[0]}) para CV con {n_cv_splits} splits en {name}.\")\n",
    "            valid_cv = False\n",
    "\n",
    "        if not valid_cv:\n",
    "            best_params_str = \"CV skipped (pocas muestras/clases)\"\n",
    "            best_score_str = \"N/A (CV skipped)\"\n",
    "            best_models[name] = None\n",
    "            with open(hyper_file, 'a') as hf:\n",
    "                hf.write(f\"{name.upper()} best params: {best_params_str}\\n\")\n",
    "                hf.write(f\"{name.upper()} best CV ROC-AUC: {best_score_str}\\n\\n\")\n",
    "            print(f\"  ! Hiperparámetros (o estado de error) guardados para {name.upper()}\")\n",
    "            continue\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            pipe, param_grid, cv=n_cv_splits, scoring='roc_auc', \n",
    "            n_jobs=1, verbose=1, refit=True, error_score='raise' \n",
    "        )\n",
    "        \n",
    "        best_estimator_for_model = None\n",
    "        try:\n",
    "            grid.fit(X_train, y_train_processed, **fit_params_grid) \n",
    "            best_estimator_for_model = grid.best_estimator_\n",
    "            best_params_str = str(grid.best_params_)\n",
    "            best_score_str = f\"{grid.best_score_:.4f}\"\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"Error de ValueError (posiblemente CV) durante GridSearchCV para {name} en {exp_name}: {ve}\", exc_info=True)\n",
    "            print(f\"Error de ValueError durante GridSearchCV para {name}, saltando este modelo: {ve}\")\n",
    "            best_params_str = \"Error en CV\"\n",
    "            best_score_str = \"Error en CV\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error general durante GridSearchCV para {name} en {exp_name}: {e}\", exc_info=True)\n",
    "            print(f\"Error general durante GridSearchCV para {name}, saltando este modelo: {e}\")\n",
    "            best_params_str = \"Error general\"\n",
    "            best_score_str = \"Error general\"\n",
    "\n",
    "        if best_estimator_for_model:\n",
    "            if name == 'nn':\n",
    "                scaler_nn = best_estimator_for_model.named_steps['scaler']\n",
    "                keras_model = best_estimator_for_model.named_steps['clf'].model\n",
    "                scaler_path = os.path.join(output_dir, f\"{exp_name}_nn_scaler.joblib\")\n",
    "                keras_path  = os.path.join(output_dir, f\"{exp_name}_nn_model.h5\")\n",
    "                joblib.dump(scaler_nn, scaler_path)\n",
    "                keras_model.save(keras_path)\n",
    "                best_models[name] = (scaler_path, keras_path)\n",
    "                print(f\"  ✔ NN scaler guardado: {scaler_path}\")\n",
    "                print(f\"  ✔ NN model guardado: {keras_path}\")\n",
    "            else:\n",
    "                pipe_skl = best_estimator_for_model\n",
    "                path = os.path.join(output_dir, f\"{exp_name}_{name}_pipeline.joblib\")\n",
    "                joblib.dump(pipe_skl, path)\n",
    "                best_models[name] = pipe_skl\n",
    "                print(f\"  ✔ {name} pipeline guardado: {path}\")\n",
    "        else:\n",
    "            best_models[name] = None\n",
    "            logging.warning(f\"No se pudo obtener best_estimator para {name} debido a un error previo.\")\n",
    "\n",
    "        with open(hyper_file, 'a') as hf:\n",
    "            hf.write(f\"{name.upper()} best params: {best_params_str}\\n\")\n",
    "            hf.write(f\"{name.upper()} best CV ROC-AUC: {best_score_str}\\n\\n\")\n",
    "        print(f\"  ✔ Params guardados para {name.upper()}\")\n",
    "        \n",
    "    return {k: v for k, v in best_models.items() if v is not None}\n",
    "\n",
    "def evaluate_and_save_reports(models, X_test, y_test, output_dir):\n",
    "    report_file = os.path.join(output_dir, 'classification_reports.txt')\n",
    "    with open(report_file, 'w') as rf:\n",
    "        rf.write(f\"Classification reports for {os.path.basename(output_dir)}\\n\")\n",
    "        rf.write(\"=\"*60 + \"\\n\\n\")\n",
    "    print(f\"\\n> Reports file: {report_file}\")\n",
    "\n",
    "    if X_test.empty or y_test.empty:\n",
    "        logging.error(\"X_test o y_test están vacíos. No se pueden generar reportes.\")\n",
    "        print(\"X_test o y_test están vacíos. Saltando evaluación.\")\n",
    "        return\n",
    "\n",
    "    y_test_processed = y_test.astype(int)\n",
    "\n",
    "    for name, mdl_or_paths in models.items():\n",
    "        print(f\"\\n> Evaluando {name.upper()}...\")\n",
    "        logging.info(f\"Evaluando {name.upper()}...\")\n",
    "        y_pred, y_prob = None, None \n",
    "\n",
    "        try:\n",
    "            if name == 'nn':\n",
    "                scaler_path, keras_path = mdl_or_paths\n",
    "                scaler_loaded = joblib.load(scaler_path)\n",
    "                model_loaded  = load_model(keras_path, compile=False)\n",
    "                Xs_eval = scaler_loaded.transform(X_test)\n",
    "                \n",
    "                probs_eval = model_loaded.predict(Xs_eval)\n",
    "                if probs_eval.ndim == 1 or probs_eval.shape[1] == 1: # Output sigmoide\n",
    "                    y_prob = probs_eval.flatten()\n",
    "                    y_pred = (y_prob > 0.5).astype(int)\n",
    "                elif probs_eval.shape[1] == 2: # Output softmax\n",
    "                    y_prob = probs_eval[:,1]\n",
    "                    y_pred = np.argmax(probs_eval, axis=1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Forma de salida inesperada del modelo NN: {probs_eval.shape}\")\n",
    "\n",
    "            else: \n",
    "                model_pipeline_loaded = mdl_or_paths\n",
    "                probs_pipeline = model_pipeline_loaded.predict_proba(X_test)\n",
    "                y_pred = model_pipeline_loaded.predict(X_test)\n",
    "                \n",
    "                if probs_pipeline.shape[1] == 2:\n",
    "                    y_prob = probs_pipeline[:, 1]\n",
    "                elif probs_pipeline.shape[1] == 1: \n",
    "                    # Esto puede pasar si el clasificador solo ve una clase durante el fit (muy raro con class_weight)\n",
    "                    # o si el modelo es inherentemente para una sola salida de probabilidad (no común para predict_proba)\n",
    "                    logging.warning(f\"predict_proba para {name} devolvió una sola columna. Asumiendo probabilidad de clase positiva.\")\n",
    "                    y_prob = probs_pipeline[:,0] # Asumir que es la probabilidad de la clase positiva\n",
    "                else:\n",
    "                    raise ValueError(f\"Forma de salida inesperada de predict_proba para {name}: {probs_pipeline.shape}\")\n",
    "\n",
    "\n",
    "            auc = roc_auc_score(y_test_processed, y_prob)\n",
    "            rpt_str = classification_report(\n",
    "                y_test_processed, y_pred,\n",
    "                target_names=['No Fraude','Fraude'],\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n{rpt_str}\")\n",
    "            with open(report_file, 'a') as rf:\n",
    "                rf.write(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n\")\n",
    "                rf.write(rpt_str + \"\\n\")\n",
    "                rf.write(\"=\"*60 + \"\\n\")\n",
    "            print(f\"  ✔ Reporte guardado para {name.upper()}\")\n",
    "        \n",
    "        except Exception as e_eval:\n",
    "            logging.error(f\"Error durante la evaluación de {name} en {output_dir}: {e_eval}\", exc_info=True)\n",
    "            print(f\"Error durante la evaluación de {name}: {e_eval}\")\n",
    "            with open(report_file, \"a\") as rf:\n",
    "                rf.write(f\"{name.upper()} - ERROR EN EVALUACIÓN: {e_eval}\\n{'-'*60}\\n\")\n",
    "# -------------------------------------------------------------------\n",
    "# Bloque principal\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    CSV_PATH    = \"C:/Users/saave/Desktop/Master_Thesis/Credit_card_data/creditcard.csv\"\n",
    "    BASE_OUTPUT = \"C:/Users/saave/Desktop/data_balance/Result_NearMiss\" # Nombre de carpeta actualizado\n",
    "    TECHNIQUE_NAME = \"NearMiss\"\n",
    "\n",
    "    for scenario_type in [\"scenario1\", \"scenario2\"]:\n",
    "        experiment_name   = f\"{TECHNIQUE_NAME}_Scaling_{scenario_type}\" \n",
    "        output_directory = os.path.join(BASE_OUTPUT, experiment_name)\n",
    "        log_file_path   = os.path.join(output_directory, f\"run_{experiment_name}.log\")\n",
    "\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        setup_logging(log_file_path)\n",
    "        \n",
    "        logging.info(f\"=================================================\")\n",
    "        logging.info(f\"=== Iniciando experimento: {experiment_name} ===\")\n",
    "        logging.info(f\"Técnica: {TECHNIQUE_NAME} con Escalado Refinado (Idea para Scen1)\")\n",
    "        logging.info(f\"Output directory: {output_directory}\")\n",
    "        logging.info(f\"=================================================\")\n",
    "        print(f\"\\n=== Iniciando: {experiment_name} ===\")\n",
    "\n",
    "\n",
    "        df_original_unscaled = load_and_prepare(CSV_PATH)\n",
    "        logging.info(f\"Dataset cargado (sin escalar). Forma: {df_original_unscaled.shape}. Clases: {dict(df_original_unscaled['Clase'].value_counts())}\")\n",
    "\n",
    "        # Validaciones básicas del DataFrame\n",
    "        if not isinstance(df_original_unscaled, pd.DataFrame) or df_original_unscaled.empty:\n",
    "            logging.error(f\"El DataFrame cargado está vacío o no es un DataFrame. Abortando {experiment_name}.\")\n",
    "            continue\n",
    "        if 'Clase' not in df_original_unscaled.columns:\n",
    "            logging.error(f\"La columna 'Clase' no se encuentra en el DataFrame. Abortando {experiment_name}.\")\n",
    "            continue\n",
    "        if df_original_unscaled.shape[0] < 10: \n",
    "            logging.error(f\"Dataset con muy pocas filas ({df_original_unscaled.shape[0]}). Abortando {experiment_name}.\")\n",
    "            continue\n",
    "        class_counts_original = df_original_unscaled['Clase'].value_counts()\n",
    "        if len(class_counts_original) < 2 or class_counts_original.get(0,0) == 0 or class_counts_original.get(1,0) == 0:\n",
    "            logging.error(f\"Dataset no tiene ambas clases o una clase está vacía: {class_counts_original.to_dict()}. Abortando {experiment_name}.\")\n",
    "            continue\n",
    "\n",
    "        X_train_final_data, X_test_final_data, y_train_final_data, y_test_final_data = run_scenario_nearmiss_refined_scaling(df_original_unscaled.copy(), scenario_type)\n",
    "\n",
    "        # Comprobaciones después de run_scenario\n",
    "        if X_train_final_data.empty or (y_train_final_data is not None and y_train_final_data.empty):\n",
    "            logging.error(f\"X_train o y_train vacíos para {experiment_name} DESPUÉS de run_scenario. Saltando entrenamiento.\")\n",
    "            continue\n",
    "        \n",
    "        # Si y_train_final_data es None (puede pasar si X_train_final_data se vuelve vacío), no podemos hacer value_counts\n",
    "        if y_train_final_data is not None and not y_train_final_data.empty:\n",
    "            y_train_counts = y_train_final_data.value_counts()\n",
    "            if len(y_train_counts) < 1: \n",
    "                logging.warning(f\"y_train no tiene muestras para {experiment_name}. El entrenamiento podría fallar.\")\n",
    "            elif len(y_train_counts) < 2:\n",
    "                logging.warning(f\"y_train tiene solo una clase para {experiment_name}. GridSearchCV se adaptará o podría fallar.\")\n",
    "        elif y_train_final_data is None or y_train_final_data.empty : # Chequeo explícito de None o vacío\n",
    "             logging.error(f\"y_train_final_data es None o está vacía para {experiment_name}. Saltando entrenamiento.\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        start_training_time = time.time()\n",
    "        trained_models_dict = train_and_save_models(X_train_final_data, y_train_final_data, experiment_name, output_directory)\n",
    "        logging.info(f\"Entrenamiento para {experiment_name} completado en {time.time() - start_training_time:.2f}s\")\n",
    "\n",
    "        if not trained_models_dict:\n",
    "            logging.warning(f\"No se entrenaron modelos exitosamente para {experiment_name}. Saltando evaluación.\")\n",
    "        else:\n",
    "            if X_test_final_data is not None and not X_test_final_data.empty and \\\n",
    "               y_test_final_data is not None and not y_test_final_data.empty:\n",
    "                 evaluate_and_save_reports(trained_models_dict, X_test_final_data, y_test_final_data, output_directory)\n",
    "            else:\n",
    "                logging.warning(f\"X_test o y_test están vacíos o son None para {experiment_name}. Saltando evaluación.\")\n",
    "        \n",
    "        logging.info(f\"=== Fin experimento {experiment_name} ===\\n\")\n",
    "        print(f\"=== Fin: {experiment_name} ===\\n\")\n",
    "\n",
    "    print(f\"Todos los experimentos {TECHNIQUE_NAME} con escalado refinado han finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
