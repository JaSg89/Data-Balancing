{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2cadc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import joblib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf # Importado para tf.random.set_seed\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    # StratifiedKFold, # No se usa directamente, GridSearchCV lo maneja\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "# --- CONFIGURACIÓN DE HILOS DE TENSORFLOW ---\n",
    "\n",
    "try:\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    logging.info(\"TensorFlow inter-op parallelism threads set to 1.\")\n",
    "    logging.info(\"TensorFlow intra-op parallelism threads set to 1.\")\n",
    "except RuntimeError as e:\n",
    "    logging.warning(f\"Could not set TensorFlow threading: {e}. This might be okay if already configured.\")\n",
    "    # Esto puede ocurrir si los hilos ya fueron configurados (ej. en un entorno interactivo como Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Configuración global\n",
    "# -------------------------------------------------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "LOG_FMT = \"%(asctime)s %(levelname)-8s %(message)s\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones auxiliares\n",
    "# -------------------------------------------------------------------\n",
    "def setup_logging(log_path):\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    root_logger = logging.getLogger()\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "        \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=LOG_FMT,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path, mode='w'),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def load_and_prepare(csv_path): # YA NO ESCALA INTERNAMENTE\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.rename(columns={'Time':'Tiempo','Amount':'Cantidad','Class':'Clase'})\n",
    "    return df\n",
    "\n",
    "def build_nn_model(n_inputs, learning_rate=0.001, dropout_rate=0.5): # Nombre consistente\n",
    "    model = Sequential([\n",
    "        Dense(32, input_shape=(n_inputs,), activation='relu'), # Como en tu script original\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Función principal de escenario (PARA SMOTE con escalado controlado)\n",
    "# -------------------------------------------------------------------\n",
    "def run_scenario_smote_controlled_scaling(df_original_unscaled, scenario):\n",
    "    target_col_name = 'Clase'\n",
    "    # Asegurarse de que X_original_unscaled es un DataFrame y y_original es una Serie\n",
    "    if not isinstance(df_original_unscaled, pd.DataFrame):\n",
    "        raise TypeError(\"df_original_unscaled debe ser un DataFrame de Pandas.\")\n",
    "    \n",
    "    X_original_unscaled = df_original_unscaled.drop(target_col_name, axis=1, errors='ignore')\n",
    "    if target_col_name not in df_original_unscaled.columns:\n",
    "        raise ValueError(f\"La columna objetivo '{target_col_name}' no se encuentra en df_original_unscaled.\")\n",
    "    y_original = df_original_unscaled[target_col_name]\n",
    "\n",
    "    smote_sampler = SMOTE(random_state=42)\n",
    "    logging.info(f\"Usando SMOTE con escalado controlado.\")\n",
    "\n",
    "    X_train_final, X_test_final, y_train_final, y_test_final = [pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')]\n",
    "\n",
    "    if scenario == 'scenario1':\n",
    "        # Escenario 1 SMOTE: Fuga de Datos INTENCIONAL\n",
    "        logging.info(f\">> ESCENARIO 1 (SMOTE): Escalado Global PRE-SMOTE -> SMOTE en TODO el dataset escalado -> Split\")\n",
    "        \n",
    "        # 1. Cargar datos (hecho) -> X_original_unscaled, y_original\n",
    "\n",
    "        # 2. Escalado Global PRE-SMOTE\n",
    "        if X_original_unscaled.empty:\n",
    "            logging.error(\"   SMOTE Scen1: X_original_unscaled está vacío. Abortando escenario.\")\n",
    "            return X_train_final, X_test_final, y_train_final, y_test_final # Devuelve vacíos\n",
    "\n",
    "        scaler_s1_global_temp = MinMaxScaler()\n",
    "        # Usar .copy() para evitar modificar X_original_unscaled si se reutiliza\n",
    "        X_global_scaled_for_smote = pd.DataFrame(\n",
    "            scaler_s1_global_temp.fit_transform(X_original_unscaled.copy()), \n",
    "            columns=X_original_unscaled.columns,\n",
    "            index=X_original_unscaled.index\n",
    "        )\n",
    "        logging.info(f\"   SMOTE Scen1: Datos originales escalados globalmente para SMOTE. Forma: {X_global_scaled_for_smote.shape}\")\n",
    "\n",
    "        # 3. Balanceo con SMOTE (Sobre Datos Globalmente Escalados)\n",
    "        X_res_smote_global_scaled = pd.DataFrame() # Inicializar\n",
    "        y_res_smote = pd.Series(dtype='float64')   # Inicializar\n",
    "\n",
    "        try:\n",
    "            if X_global_scaled_for_smote.empty or y_original.empty:\n",
    "                logging.warning(\"   SMOTE Scen1: No hay datos para SMOTE. Usando datos escalados globalmente sin balanceo.\")\n",
    "                X_res_smote_global_scaled = X_global_scaled_for_smote\n",
    "                y_res_smote = y_original\n",
    "            else:\n",
    "                # SMOTE devuelve arrays NumPy\n",
    "                X_res_np, y_res_np = smote_sampler.fit_resample(X_global_scaled_for_smote.to_numpy(), y_original.to_numpy())\n",
    "                X_res_smote_global_scaled = pd.DataFrame(X_res_np, columns=X_global_scaled_for_smote.columns)\n",
    "                y_res_smote = pd.Series(y_res_np, name=y_original.name)\n",
    "            logging.info(f\"   SMOTE Scen1: Tamaño después de SMOTE (en datos escalados globalmente): {X_res_smote_global_scaled.shape}, Distribución y_res: {dict(y_res_smote.value_counts())}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"   SMOTE Scen1: Error durante SMOTE.fit_resample: {e}. Usando datos escalados globalmente sin balanceo.\")\n",
    "            X_res_smote_global_scaled = X_global_scaled_for_smote # Fallback\n",
    "            y_res_smote = y_original\n",
    "\n",
    "        if X_res_smote_global_scaled.empty or y_res_smote.empty:\n",
    "            logging.error(\"   SMOTE Scen1: X_res o y_res vacíos después del balanceo/fallback. Abortando escenario.\")\n",
    "            return X_train_final, X_test_final, y_train_final, y_test_final # Devuelve vacíos\n",
    "        \n",
    "        # 4. Split (División del Conjunto Balanceado y Globalmente Escalado)\n",
    "        # Los datos (X_train_final, X_test_final) ya están escalados por el scaler_s1_global_temp\n",
    "        stratify_param_s1 = y_res_smote if not y_res_smote.empty and len(y_res_smote.unique()) > 1 else None\n",
    "        X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "            X_res_smote_global_scaled, y_res_smote, test_size=0.2, stratify=stratify_param_s1, random_state=42, shuffle=True\n",
    "        )\n",
    "        logging.info(f\"   SMOTE Scen1: Después de Split. X_train_final (escalado): {X_train_final.shape}, X_test_final (escalado): {X_test_final.shape}\")\n",
    "\n",
    "    else: # scenario2 (Sin Fuga de Datos)\n",
    "        # 1. Split del dataset ORIGINAL (df_original_unscaled)\n",
    "        logging.info(f\">> ESCENARIO 2 (SMOTE): Split del dataset (sin escalar) -> Escalado Separado LIMPIO -> SMOTE (solo en train escalado)\")\n",
    "        stratify_param_s2_initial = y_original if not y_original.empty and len(y_original.unique()) > 1 else None\n",
    "        X_train_raw, X_test_raw, y_train_orig, y_test_final_orig = train_test_split(\n",
    "            X_original_unscaled, y_original, test_size=0.2, stratify=stratify_param_s2_initial, random_state=42, shuffle=True\n",
    "        )\n",
    "        logging.info(f\"   SMOTE Scen2: Después de Split inicial. X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n",
    "\n",
    "        # 2. Escalado SEPARADO y LIMPIO para modelado (y para SMOTE en train)\n",
    "        X_train_scaled_for_smote = pd.DataFrame()\n",
    "        y_test_final = y_test_final_orig # y_test no cambia\n",
    "\n",
    "        if not X_train_raw.empty:\n",
    "            scaler_s2_modelado = MinMaxScaler()\n",
    "            X_train_scaled_for_smote = pd.DataFrame(scaler_s2_modelado.fit_transform(X_train_raw), columns=X_train_raw.columns, index=X_train_raw.index)\n",
    "            if not X_test_raw.empty:\n",
    "                X_test_final = pd.DataFrame(scaler_s2_modelado.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            logging.info(f\"   SMOTE Scen2: Después de Escalado LIMPIO. X_train_scaled_for_smote: {X_train_scaled_for_smote.shape}, X_test_final: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "        \n",
    "        # 3. SMOTE solo en train (que ya está escalado para modelado)\n",
    "        X_train_final = X_train_scaled_for_smote.copy() # Inicializar por si SMOTE falla\n",
    "        y_train_final = y_train_orig.copy()            # Inicializar\n",
    "\n",
    "        if not X_train_scaled_for_smote.empty and not y_train_orig.empty:\n",
    "            try:\n",
    "                # Comprobar si hay al menos una clase minoritaria para SMOTE\n",
    "                if len(y_train_orig.value_counts().get(1, [])) < smote_sampler.k_neighbors +1 and len(y_train_orig.value_counts()) > 1 : #SMOTE necesita k+1 muestras de la minoría\n",
    "                     logging.warning(f\"   SMOTE Scen2: No hay suficientes muestras en la clase minoritaria del train ({len(y_train_orig.value_counts().get(1,[]))}) para SMOTE con k_neighbors={smote_sampler.k_neighbors}. Usando train escalado sin balanceo.\")\n",
    "                     # X_train_final y y_train_final ya están seteados a los datos pre-balanceo\n",
    "                elif len(y_train_orig.unique()) < 2:\n",
    "                    logging.warning(\"   SMOTE Scen2: Solo una clase presente en y_train_orig. SMOTE no se aplicará.\")\n",
    "                    # X_train_final y y_train_final ya están seteados\n",
    "                else:\n",
    "                    X_res_np_train, y_res_np_train = smote_sampler.fit_resample(X_train_scaled_for_smote.to_numpy(), y_train_orig.to_numpy())\n",
    "                    X_train_final = pd.DataFrame(X_res_np_train, columns=X_train_scaled_for_smote.columns)\n",
    "                    y_train_final = pd.Series(y_res_np_train, name=y_train_orig.name)\n",
    "                logging.info(f\"   SMOTE Scen2: Después de SMOTE en train. X_train_final (escalado): {X_train_final.shape}, y_train_final dist: {dict(y_train_final.value_counts() if not y_train_final.empty else {})}\")\n",
    "            except ValueError as e:\n",
    "                logging.error(f\"   SMOTE Scen2: Error durante SMOTE.fit_resample en train: {e}. Usando train escalado sin balanceo.\")\n",
    "                # X_train_final y y_train_final ya están seteados a los datos pre-balanceo\n",
    "        \n",
    "    logging.info(f\"   → Tamaños finales para modelado. Train: {X_train_final.shape if not X_train_final.empty else '(empty)'}, Test: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "    if not (y_train_final is None or y_train_final.empty):\n",
    "        logging.info(f\"   → Distribución y_train_final: {dict(y_train_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_train_final está vacío o es None.\")\n",
    "    if not (y_test_final is None or y_test_final.empty):\n",
    "        logging.info(f\"   → Distribución y_test_final: {dict(y_test_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_test_final está vacío o es None.\")\n",
    "        \n",
    "    return X_train_final, X_test_final, y_train_final, y_test_final\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones de entrenamiento y evaluación (Prácticamente sin cambios de tu script original de SMOTE)\n",
    "# -------------------------------------------------------------------\n",
    "def train_and_save_models(X_train, y_train, exp_name, output_dir):\n",
    "    hyper_file = os.path.join(output_dir, 'hyperparameters.txt')\n",
    "    with open(hyper_file, 'w') as hf:\n",
    "        hf.write(f\"Hyperparameters for experiment {exp_name}\\n\")\n",
    "        hf.write(\"=\"*60 + \"\\n\\n\")\n",
    "    print(f\"> Hyperparameters will be saved to: {hyper_file}\")\n",
    "\n",
    "    if X_train.empty or X_train.shape[1] == 0:\n",
    "        logging.error(f\"X_train está vacío o no tiene características ANTES de entrenar modelos para {exp_name}. Saltando entrenamiento.\")\n",
    "        return {}\n",
    "\n",
    "    n_inputs = X_train.shape[1]\n",
    "\n",
    "    nn_wrapper = KerasClassifier(\n",
    "        build_fn=build_nn_model,\n",
    "        n_inputs=n_inputs,\n",
    "        verbose=0\n",
    "    )\n",
    "    # Grillas de hiperparámetros de tu script original de SMOTE (ajustadas ligeramente)\n",
    "    specs = {\n",
    "        \n",
    "        'nn': (\n",
    "            nn_wrapper,\n",
    "            {\n",
    "                'clf__learning_rate':[0.0001], \n",
    "                'clf__dropout_rate':[0.3],   \n",
    "                'clf__batch_size':[32],      \n",
    "                'clf__epochs':[75],           \n",
    "                'clf__validation_split': [0.1]   \n",
    "            }\n",
    "        ),\n",
    "\n",
    "        'logreg': (\n",
    "            LogisticRegression(random_state=42, max_iter=1000), \n",
    "            {'clf__penalty':['l1','l2'], 'clf__C':[0.1], 'clf__solver':['liblinear']}\n",
    "        ),\n",
    "        'svm': (\n",
    "            SVC(probability=True, random_state=42, class_weight='balanced'), \n",
    "            {'clf__C':[1, 10], 'clf__kernel':['linear']} \n",
    "        ),\n",
    "        'rf': (\n",
    "            RandomForestClassifier(random_state=42), \n",
    "            {'clf__n_estimators':[200], 'clf__max_depth':[20, None]} \n",
    "        ),\n",
    "        'xgb': (\n",
    "            XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "            {'clf__n_estimators':[100], 'clf__max_depth':[10]} \n",
    "        )\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, (clf, param_grid) in specs.items():\n",
    "        print(f\"\\n> Entrenando {name.upper()}...\")\n",
    "        logging.info(f\"Entrenando {name.upper()}...\")\n",
    "        \n",
    "        pipe = ImbPipeline([\n",
    "            ('scaler', MinMaxScaler()),\n",
    "            ('clf', clf)\n",
    "        ])\n",
    "        \n",
    "        y_train_processed = y_train.astype(int)\n",
    "\n",
    "        fit_params_grid = {}\n",
    "        if name == 'nn':\n",
    "            # Callbacks ya están en la definición de nn_wrapper\n",
    "            pass # validation_split se pasa a través de param_grid\n",
    "\n",
    "\n",
    "        n_cv_splits = 5 # Como en tu script de SMOTE\n",
    "        min_samples_for_cv = n_cv_splits \n",
    "        \n",
    "        valid_cv = True\n",
    "        if len(y_train_processed.unique()) > 1:\n",
    "            min_class_count = min(y_train_processed.value_counts())\n",
    "            if min_class_count < n_cv_splits:\n",
    "                logging.warning(f\"Clase minoritaria en y_train para {name} tiene {min_class_count} muestras, menos que n_splits={n_cv_splits}. Intentando con n_splits={max(2, min_class_count)}.\")\n",
    "                n_cv_splits = max(2, min_class_count) \n",
    "                min_samples_for_cv = n_cv_splits\n",
    "        else: \n",
    "            logging.warning(f\"Solo una clase en y_train para {name}. CV no es posible.\")\n",
    "            valid_cv = False\n",
    "        \n",
    "        if X_train.shape[0] < min_samples_for_cv:\n",
    "            logging.warning(f\"No hay suficientes muestras en X_train ({X_train.shape[0]}) para CV con {n_cv_splits} splits en {name}.\")\n",
    "            valid_cv = False\n",
    "\n",
    "        if not valid_cv:\n",
    "            best_params_str = \"CV skipped (pocas muestras/clases)\"\n",
    "            best_score_str = \"N/A (CV skipped)\"\n",
    "            best_models[name] = None \n",
    "            with open(hyper_file, 'a') as hf:\n",
    "                hf.write(f\"{name.upper()} best params: {best_params_str}\\n\")\n",
    "                hf.write(f\"{name.upper()} best CV ROC-AUC: {best_score_str}\\n\\n\")\n",
    "            print(f\"  ! Hiperparámetros (o estado de error) guardados para {name.upper()}\")\n",
    "            continue\n",
    "        \n",
    "        cv_obj = StratifiedKFold( n_splits=min(4, min_class_count), shuffle=True, random_state=42)\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            pipe, param_grid,\n",
    "            cv=cv_obj, scoring='f1', \n",
    "            n_jobs=5, # Cambiado de 2 a 1 por consistencia y evitar problemas con Keras\n",
    "            verbose=2, refit=True, error_score='raise'\n",
    "        )\n",
    "        \n",
    "        best_estimator_for_model = None\n",
    "        try:\n",
    "            # Para NN, los callbacks están en nn_wrapper, validation_split en param_grid\n",
    "            grid.fit(X_train, y_train_processed) \n",
    "            best_estimator_for_model = grid.best_estimator_\n",
    "            best_params_str = str(grid.best_params_)\n",
    "            best_score_str = f\"{grid.best_score_:.4f}\"\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"Error de ValueError (posiblemente CV) durante GridSearchCV para {name} en {exp_name}: {ve}\", exc_info=True)\n",
    "            best_params_str = \"Error en CV\"\n",
    "            best_score_str = \"Error en CV\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error general durante GridSearchCV para {name} en {exp_name}: {e}\", exc_info=True)\n",
    "            best_params_str = \"Error general\"\n",
    "            best_score_str = \"Error general\"\n",
    "\n",
    "        if best_estimator_for_model:\n",
    "            if name == 'nn':\n",
    "                scaler_nn = best_estimator_for_model.named_steps['scaler']\n",
    "                keras_model_nn = best_estimator_for_model.named_steps['clf'].model \n",
    "                \n",
    "                scaler_path = os.path.join(output_dir, f\"{exp_name}_{name}_scaler.joblib\")\n",
    "                keras_model_path = os.path.join(output_dir, f\"{exp_name}_{name}_keras_model.h5\")\n",
    "                \n",
    "                joblib.dump(scaler_nn, scaler_path)\n",
    "                keras_model_nn.save(keras_model_path)\n",
    "                \n",
    "                print(f\"  ✔ Scaler NN guardado: {scaler_path}\")\n",
    "                print(f\"  ✔ Modelo Keras NN guardado: {keras_model_path}\")\n",
    "                best_models[name] = (scaler_path, keras_model_path)\n",
    "            else:\n",
    "                model_path = os.path.join(output_dir, f\"{exp_name}_{name}_best_pipeline.joblib\")\n",
    "                joblib.dump(best_estimator_for_model, model_path)\n",
    "                print(f\"  ✔ Modelo (pipeline) guardado: {model_path}\")\n",
    "                best_models[name] = best_estimator_for_model\n",
    "        else:\n",
    "            best_models[name] = None\n",
    "            logging.warning(f\"No se pudo obtener best_estimator para {name} debido a un error previo.\")\n",
    "\n",
    "        with open(hyper_file, 'a') as hf:\n",
    "            hf.write(f\"{name.upper()} best params: {best_params_str}\\n\")\n",
    "            hf.write(f\"{name.upper()} best CV ROC-AUC: {best_score_str}\\n\\n\")\n",
    "        print(f\"  ✔ Hiperparámetros guardados para {name.upper()}\")\n",
    "        \n",
    "    return {k:v for k,v in best_models.items() if v is not None}\n",
    "\n",
    "\n",
    "def evaluate_and_save_reports(models, X_test, y_test, output_dir):\n",
    "    # ... (Esta función puede ser idéntica a la de la versión NearMiss_RefinedScaling)\n",
    "    report_file = os.path.join(output_dir, \"classification_reports.txt\")\n",
    "    with open(report_file, \"w\") as rf:\n",
    "        rf.write(f\"Classification reports for {os.path.basename(output_dir)}\\n\")\n",
    "        rf.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    print(f\"\\n> Reports will be saved to: {report_file}\")\n",
    "\n",
    "    if X_test.empty or y_test.empty:\n",
    "        logging.error(\"X_test o y_test están vacíos. No se pueden generar reportes.\")\n",
    "        return\n",
    "\n",
    "    y_test_processed = y_test.astype(int)\n",
    "\n",
    "    for name, model_or_paths in models.items():\n",
    "        print(f\"\\n> Evaluando {name.upper()}...\")\n",
    "        logging.info(f\"Evaluando {name.upper()}...\")\n",
    "        y_pred, y_prob = None, None \n",
    "\n",
    "        try:\n",
    "            if name == \"nn\":\n",
    "                scaler_path, keras_model_path = model_or_paths\n",
    "                scaler = joblib.load(scaler_path)\n",
    "                keras_model = load_model(keras_model_path, compile=False) \n",
    "                # X_test ya está escalado. El scaler cargado es el del pipeline de entrenamiento.\n",
    "                X_test_for_eval = scaler.transform(X_test) \n",
    "                \n",
    "                y_prob_all_classes = keras_model.predict(X_test_for_eval)\n",
    "\n",
    "                if y_prob_all_classes.ndim == 1 or y_prob_all_classes.shape[1] == 1: \n",
    "                    y_prob = y_prob_all_classes.flatten()\n",
    "                    y_pred = (y_prob > 0.5).astype(int)\n",
    "                elif y_prob_all_classes.shape[1] == 2: \n",
    "                    y_prob = y_prob_all_classes[:, 1]\n",
    "                    y_pred = np.argmax(y_prob_all_classes, axis=1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Forma de salida inesperada del modelo NN: {y_prob_all_classes.shape}\")\n",
    "\n",
    "            else: \n",
    "                model_pipeline = model_or_paths\n",
    "                # El pipeline se encarga de escalar X_test\n",
    "                y_prob_all_classes = model_pipeline.predict_proba(X_test)\n",
    "                y_pred = model_pipeline.predict(X_test)\n",
    "                \n",
    "                if y_prob_all_classes.shape[1] == 2:\n",
    "                    y_prob = y_prob_all_classes[:, 1]\n",
    "                elif y_prob_all_classes.shape[1] == 1:\n",
    "                    logging.warning(f\"predict_proba para {name} devolvió una sola columna. Asumiendo probabilidad de clase positiva.\")\n",
    "                    y_prob = y_prob_all_classes[:,0]\n",
    "                else:\n",
    "                    raise ValueError(f\"Forma de salida inesperada de predict_proba para {name}: {y_prob_all_classes.shape}\")\n",
    "\n",
    "\n",
    "            auc = roc_auc_score(y_test_processed, y_prob)\n",
    "            report_str = classification_report(\n",
    "                y_test_processed, y_pred,\n",
    "                target_names=[\"No Fraude\", \"Fraude\"],\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n{report_str}\")\n",
    "\n",
    "            with open(report_file, \"a\") as rf:\n",
    "                rf.write(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n\")\n",
    "                rf.write(report_str + \"\\n\")\n",
    "                rf.write(\"-\" * 60 + \"\\n\") # Consistente con tu script original\n",
    "            print(f\"  ✔ Reporte guardado para {name.upper()}\")\n",
    "        \n",
    "        except Exception as e_eval:\n",
    "            logging.error(f\"Error durante la evaluación de {name} en {output_dir}: {e_eval}\", exc_info=True)\n",
    "            print(f\"Error durante la evaluación de {name}: {e_eval}\")\n",
    "            with open(report_file, \"a\") as rf:\n",
    "                rf.write(f\"{name.upper()} - ERROR EN EVALUACIÓN: {e_eval}\\n{'-'*60}\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Bloque principal\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    CSV_PATH = \"/scratch/sivar/jarevalo/jsaavedra/creditcard.csv\"\n",
    "    BASE_OUTPUT = \"/scratch/sivar/jarevalo/jsaavedra/resultados_SMOTE\" \n",
    "\n",
    "    TECHNIQUE_NAME = \"SMOTE\" # Tu técnica\n",
    "\n",
    "    for scenario_type in [\"scenario1\", \"scenario2\"]:\n",
    "        experiment_name   = f\"{TECHNIQUE_NAME}_Scaling_{scenario_type}\" \n",
    "        output_dir = os.path.join(BASE_OUTPUT, experiment_name) # <--- Variable definida como output_dir\n",
    "        log_file_path   = os.path.join(output_dir, f\"run_{experiment_name}.log\")\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        setup_logging(log_file_path)\n",
    "        \n",
    "        logging.info(f\"=================================================\")\n",
    "        logging.info(f\"=== Iniciando experimento: {experiment_name} ===\")\n",
    "        logging.info(f\"Técnica: {TECHNIQUE_NAME} con Escalado Controlado\")\n",
    "        # logging.info(f\"Output directory: {output_directory}\") # <--- LÍNEA ORIGINAL CON ERROR\n",
    "        logging.info(f\"Output directory: {output_dir}\")      # <--- LÍNEA CORREGIDA\n",
    "        logging.info(f\"=================================================\")\n",
    "        print(f\"\\n=== Iniciando experimento: {experiment_name} ===\")\n",
    "\n",
    "        # ... el resto de tu bloque main ...\n",
    "        df_original_unscaled = load_and_prepare(CSV_PATH) # Cambiado de load_and_prepare_unscaled si solo hay una\n",
    "        logging.info(f\"Dataset cargado (sin escalar). Forma: {df_original_unscaled.shape}. Clases: {dict(df_original_unscaled['Clase'].value_counts())}\")\n",
    "\n",
    "        if not isinstance(df_original_unscaled, pd.DataFrame) or df_original_unscaled.empty:\n",
    "            logging.error(f\"El DataFrame cargado está vacío o no es un DataFrame. Abortando {experiment_name}.\")\n",
    "            continue\n",
    "        if 'Clase' not in df_original_unscaled.columns: # Asumiendo que 'Clase' es el target después de renombrar\n",
    "            logging.error(f\"La columna 'Clase' no se encuentra en el DataFrame. Abortando {experiment_name}.\")\n",
    "            continue\n",
    "        if df_original_unscaled.shape[0] < 10: \n",
    "            logging.error(f\"Dataset con muy pocas filas ({df_original_unscaled.shape[0]}). Abortando {experiment_name}.\")\n",
    "            continue\n",
    "        class_counts_original = df_original_unscaled['Clase'].value_counts()\n",
    "        if len(class_counts_original) < 2 or class_counts_original.get(0,0) == 0 or class_counts_original.get(1,0) == 0:\n",
    "            logging.error(f\"Dataset no tiene ambas clases o una clase está vacía: {class_counts_original.to_dict()}. Abortando {experiment_name}.\")\n",
    "            continue\n",
    "\n",
    "        X_train_final_data, X_test_final_data, y_train_final_data, y_test_final_data = run_scenario_smote_controlled_scaling(df_original_unscaled.copy(), scenario_type)\n",
    "\n",
    "        if X_train_final_data.empty or (y_train_final_data is not None and y_train_final_data.empty): # Comprobación robusta\n",
    "            logging.error(f\"X_train o y_train vacíos para {experiment_name} DESPUÉS de run_scenario. Saltando entrenamiento.\")\n",
    "            continue\n",
    "        \n",
    "        if y_train_final_data is not None and not y_train_final_data.empty:\n",
    "            y_train_counts = y_train_final_data.value_counts()\n",
    "            if len(y_train_counts) < 1: \n",
    "                logging.warning(f\"y_train no tiene muestras para {experiment_name}. El entrenamiento podría fallar.\")\n",
    "            elif len(y_train_counts) < 2:\n",
    "                logging.warning(f\"y_train tiene solo una clase para {experiment_name}. GridSearchCV se adaptará o podría fallar.\")\n",
    "        elif y_train_final_data is None or y_train_final_data.empty : # Comprobación más explícita\n",
    "             logging.error(f\"y_train_final_data es None o está vacía para {experiment_name}. Saltando entrenamiento.\")\n",
    "             continue\n",
    "\n",
    "        start_training_time = time.time()\n",
    "        trained_models_dict = train_and_save_models(X_train_final_data, y_train_final_data, experiment_name, output_dir) # output_dir\n",
    "        logging.info(f\"Entrenamiento para {experiment_name} completado en {time.time() - start_training_time:.2f}s\")\n",
    "\n",
    "        if not trained_models_dict:\n",
    "            logging.warning(f\"No se entrenaron modelos exitosamente para {experiment_name}. Saltando evaluación.\")\n",
    "        else:\n",
    "            if X_test_final_data is not None and not X_test_final_data.empty and \\\n",
    "               y_test_final_data is not None and not y_test_final_data.empty:\n",
    "                 evaluate_and_save_reports(trained_models_dict, X_test_final_data, y_test_final_data, output_dir) # output_dir\n",
    "            else:\n",
    "                logging.warning(f\"X_test o y_test están vacíos o son None para {experiment_name}. Saltando evaluación.\")\n",
    "        \n",
    "        logging.info(f\"=== Fin experimento {experiment_name} ===\\n\")\n",
    "        print(f\"=== Fin experimento: {experiment_name} ===\\n\")\n",
    "\n",
    "    print(f\"Todos los experimentos {TECHNIQUE_NAME} con escalado controlado han finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
