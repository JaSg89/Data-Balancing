{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04ecbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import joblib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf # Importado para tf.random.set_seed\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    # StratifiedKFold, # No se usa directamente, GridSearchCV lo maneja\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "# --- CONFIGURACIÓN DE HILOS DE TENSORFLOW ---\n",
    "\n",
    "try:\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    logging.info(\"TensorFlow inter-op parallelism threads set to 1.\")\n",
    "    logging.info(\"TensorFlow intra-op parallelism threads set to 1.\")\n",
    "except RuntimeError as e:\n",
    "    # Esto puede ocurrir si los hilos ya fueron configurados (ej. en un entorno interactivo como Jupyter)\n",
    "    logging.warning(f\"Could not set TensorFlow threading: {e}. This might be okay if already configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Configuración \n",
    "# -------------------------------------------------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "LOG_FMT = \"%(asctime)s %(levelname)-8s %(message)s\"\n",
    "N_SAMPLES_FEUS = 55000 # Número de muestras a conservar por FEUS\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones auxiliares\n",
    "# -------------------------------------------------------------------\n",
    "def setup_logging(log_path):\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    root_logger = logging.getLogger()\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=LOG_FMT,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path, mode='w'),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def load_and_prepare_unscaled(csv_path): # Renombrado para claridad\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'Clase' in df.columns and 'Class' not in df.columns:\n",
    "        df = df.rename(columns={'Time':'Tiempo','Amount':'Cantidad','Clase':'Class'})\n",
    "    elif 'Tiempo' in df.columns and 'Time' not in df.columns:\n",
    "        df = df.rename(columns={'Tiempo':'Time','Cantidad':'Amount'})\n",
    "    # NO ESCALA NADA AQUÍ\n",
    "    return df\n",
    "\n",
    "def build_nn_model(n_inputs, learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Dense(32, input_shape=(n_inputs,), activation='relu'), \n",
    "        Dropout(dropout_rate),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones de resampling FEUS con escalado controlado para Mahalanobis\n",
    "# Estas funciones devuelven datos en la escala que tenían al ENTRAR\n",
    "# (o escalados si es Scen2 y entraron escalados)\n",
    "# -------------------------------------------------------------------\n",
    "def feus_apply_logic(data_features_input: pd.DataFrame, # Características del conjunto sobre el cual operar\n",
    "                     n_samples_to_keep: int,\n",
    "                     operation_context: str # \"Scen1_Full\" o \"Scen2_Train\"\n",
    "                     ) -> pd.Index: # Devuelve los índices de data_features_input a conservar\n",
    "    \"\"\"\n",
    "    Lógica central de FEUS.\n",
    "    Escala INTERNAMENTE todas las características de 'data_features_input' para el cálculo de Mahalanobis.\n",
    "    Selecciona las n_samples_to_keep más distantes globalmente DENTRO de data_features_input.\n",
    "    Devuelve los índices de 'data_features_input' a conservar.\n",
    "    \"\"\"\n",
    "    if data_features_input.empty or data_features_input.shape[1] == 0:\n",
    "        logging.warning(f\"FEUS {operation_context}: No hay características para calcular Mahalanobis. \"\n",
    "                        f\"Devolviendo hasta {n_samples_to_keep} índices disponibles.\")\n",
    "        return data_features_input.head(min(n_samples_to_keep, len(data_features_input))).index\n",
    "\n",
    "    # Escalado INTERNO de TODAS las características para el cálculo de Mahalanobis\n",
    "    scaler_internal = MinMaxScaler()\n",
    "    # data_features_input ya es SOLO las features\n",
    "    X_scaled_for_mahalanobis = pd.DataFrame(scaler_internal.fit_transform(data_features_input),\n",
    "                                            columns=data_features_input.columns,\n",
    "                                            index=data_features_input.index)\n",
    "\n",
    "    if X_scaled_for_mahalanobis.shape[0] < 2:\n",
    "        logging.warning(f\"FEUS {operation_context}: Menos de 2 muestras para calcular covarianza. \"\n",
    "                        f\"Devolviendo hasta {n_samples_to_keep} índices disponibles.\")\n",
    "        return data_features_input.head(min(n_samples_to_keep, len(data_features_input))).index\n",
    "\n",
    "    mean_vector = X_scaled_for_mahalanobis.mean().values\n",
    "    \n",
    "    if X_scaled_for_mahalanobis.shape[0] <= X_scaled_for_mahalanobis.shape[1]:\n",
    "        logging.warning(f\"FEUS {operation_context}: Covarianza podría ser singular (muestras={X_scaled_for_mahalanobis.shape[0]}, feats={X_scaled_for_mahalanobis.shape[1]}). Usando ddof=0.\")\n",
    "        cov_matrix = np.cov(X_scaled_for_mahalanobis.values, rowvar=False, ddof=0)\n",
    "    else:\n",
    "        cov_matrix = np.cov(X_scaled_for_mahalanobis.values, rowvar=False)\n",
    "\n",
    "    try:\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    except np.linalg.LinAlgError:\n",
    "        logging.warning(f\"FEUS {operation_context}: Matriz de covarianza singular, usando pseudo-inversa.\")\n",
    "        inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "    \n",
    "    diff_values = X_scaled_for_mahalanobis.values - mean_vector\n",
    "    left = diff_values.dot(inv_cov_matrix)\n",
    "    distances_sq = np.einsum('ij,ij->i', left, diff_values)\n",
    "    distances = np.sqrt(np.maximum(distances_sq, 0))\n",
    "    \n",
    "    # Añadir distancias al DataFrame de características de ENTRADA (para usar sus índices)\n",
    "    data_features_with_dist = data_features_input.copy() # Evitar SettingWithCopyWarning\n",
    "    data_features_with_dist['Mahalanobis_Distance_Internal'] = distances\n",
    "    \n",
    "    selected_indices = data_features_with_dist.sort_values(\n",
    "        by='Mahalanobis_Distance_Internal', ascending=False\n",
    "    ).head(n_samples_to_keep).index\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "def feus_scenario1_apply(df_orig_unscaled: pd.DataFrame, target_col_name='Class', n_samples_to_keep=N_SAMPLES_FEUS) -> (pd.DataFrame, pd.Series):\n",
    "    \"\"\"\n",
    "    Aplica FEUS al dataset completo (df_orig_unscaled, que NO está escalado para modelado).\n",
    "    Escala internamente para Mahalanobis. Devuelve X_res, y_res en su escala original (unscaled).\n",
    "    \"\"\"\n",
    "    df = df_orig_unscaled.copy()\n",
    "    X_for_feus_unscaled = df.drop(columns=[target_col_name])\n",
    "    # y_original = df[target_col_name] # No se usa y_original directamente aquí, solo para reconstruir\n",
    "\n",
    "    if X_for_feus_unscaled.empty:\n",
    "        logging.warning(\"FEUS Scen1: df_orig_unscaled sin características. Devolviendo truncado si es necesario.\")\n",
    "        df_resampled = df.head(min(n_samples_to_keep, len(df)))\n",
    "        return df_resampled.drop(columns=[target_col_name]), df_resampled[target_col_name]\n",
    "\n",
    "    selected_indices = feus_apply_logic(X_for_feus_unscaled, n_samples_to_keep, \"Scen1_Full\")\n",
    "    \n",
    "    # Seleccionar del DataFrame ORIGINAL (df_orig_unscaled)\n",
    "    df_resampled = df_orig_unscaled.loc[selected_indices].reset_index(drop=True) # No es necesario sample(frac=1) aquí\n",
    "    \n",
    "    X_res_unscaled = df_resampled.drop(columns=[target_col_name])\n",
    "    y_res = df_resampled[target_col_name]\n",
    "    \n",
    "    logging.info(f\"FEUS Scen1: Resampled. Original: {df_orig_unscaled.shape[0]}, Seleccionado: {len(df_resampled)}, Objetivo: {n_samples_to_keep}\")\n",
    "    logging.info(f\"   Distribución de clases en FEUS Scen1 (después de seleccionar {n_samples_to_keep} más lejanos): {dict(y_res.value_counts())}\")\n",
    "    return X_res_unscaled, y_res\n",
    "\n",
    "\n",
    "def feus_scenario2_apply(X_train_input_scaled: pd.DataFrame, # X_train YA ESCALADO para modelado\n",
    "                         y_train_orig: pd.Series,\n",
    "                         n_samples_to_keep=N_SAMPLES_FEUS) -> (pd.DataFrame, pd.Series):\n",
    "\n",
    "    X_train_current_scaled = X_train_input_scaled.copy() # Datos ya escalados para modelado\n",
    "    y_train = y_train_orig.copy()\n",
    "\n",
    "    if X_train_current_scaled.empty:\n",
    "        logging.warning(\"FEUS Scen2: X_train_input_scaled vacío. Devolviendo truncado si es necesario.\")\n",
    "        num_to_keep_actual = min(n_samples_to_keep, len(X_train_current_scaled)) # len puede ser 0\n",
    "        return X_train_current_scaled.head(num_to_keep_actual), y_train.head(num_to_keep_actual)\n",
    "\n",
    "    selected_indices = feus_apply_logic(X_train_current_scaled, n_samples_to_keep, \"Scen2_Train\")\n",
    "    \n",
    "    # Seleccionar de los datos de entrada (que ya están escalados)\n",
    "    X_res_scaled = X_train_current_scaled.loc[selected_indices].reset_index(drop=True)\n",
    "    y_res = y_train.loc[selected_indices].reset_index(drop=True)\n",
    "\n",
    "    logging.info(f\"FEUS Scen2: Resampled train set. Original: {X_train_input_scaled.shape[0]}, Seleccionado: {len(X_res_scaled)}, Objetivo: {n_samples_to_keep}\")\n",
    "    logging.info(f\"   Distribución de clases en y_train después de FEUS Scen2: {dict(y_res.value_counts())}\")\n",
    "    return X_res_scaled, y_res\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Función principal de escenario \n",
    "# -------------------------------------------------------------------\n",
    "def run_scenario_feus_controlled_scaling(df_original_unscaled, scenario, target_col_name='Class'):\n",
    "    X_original_unscaled = df_original_unscaled.drop(target_col_name, axis=1)\n",
    "    y_original = df_original_unscaled[target_col_name]\n",
    "    \n",
    "    X_train_final, X_test_final, y_train_final, y_test_final = [None]*4 \n",
    "\n",
    "    logging.info(f\"Usando FEUS (escalado interno para Mahalanobis, escalado de modelado controlado por escenario).\")\n",
    "\n",
    "    if scenario == 'scenario1':\n",
    "        logging.info(f\">> ESCENARIO 1: FEUS en TODO el dataset (sin escalar) -> Split -> Escalado Separado\")\n",
    "        X_res_unscaled, y_res = feus_scenario1_apply(df_original_unscaled.copy(), \n",
    "                                                     target_col_name=target_col_name, \n",
    "                                                     n_samples_to_keep=N_SAMPLES_FEUS)\n",
    "        logging.info(f\"   FEUS Scen1: Después de resample (antes de split y escalar): {X_res_unscaled.shape}, Distribución y_res: {dict(y_res.value_counts())}\")\n",
    "\n",
    "        if X_res_unscaled.empty or y_res.empty:\n",
    "            logging.error(\"   FEUS Scen1: X_res_unscaled o y_res vacíos después del balanceo. Abortando escenario.\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')\n",
    "\n",
    "        stratify_param_s1 = y_res if not y_res.empty and len(y_res.unique()) > 1 else None\n",
    "        X_train_raw, X_test_raw, y_train_final, y_test_final = train_test_split(\n",
    "            X_res_unscaled, y_res, test_size=0.2, stratify=stratify_param_s1, random_state=42\n",
    "        )\n",
    "        logging.info(f\"   FEUS Scen1: Después de Split. X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n",
    "\n",
    "        if not X_train_raw.empty:\n",
    "            scaler_s1 = MinMaxScaler()\n",
    "            X_train_final = pd.DataFrame(scaler_s1.fit_transform(X_train_raw), columns=X_train_raw.columns, index=X_train_raw.index)\n",
    "            if not X_test_raw.empty:\n",
    "                X_test_final = pd.DataFrame(scaler_s1.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            else: X_test_final = pd.DataFrame()\n",
    "            logging.info(f\"   FEUS Scen1: Después de Escalado. X_train_final: {X_train_final.shape}, X_test_final: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "        else:\n",
    "            X_train_final = pd.DataFrame()\n",
    "            X_test_final = pd.DataFrame()\n",
    "\n",
    "    else: # scenario2\n",
    "        logging.info(f\">> ESCENARIO 2: Split del dataset (sin escalar) -> Escalado Separado -> FEUS (solo en train escalado)\")\n",
    "        stratify_param_s2_initial = y_original if not y_original.empty and len(y_original.unique()) > 1 else None\n",
    "        X_train_raw, X_test_raw, y_train_orig, y_test_final = train_test_split(\n",
    "            X_original_unscaled, y_original, test_size=0.2, stratify=stratify_param_s2_initial, random_state=42\n",
    "        )\n",
    "        logging.info(f\"   FEUS Scen2: Después de Split inicial. X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n",
    "\n",
    "        X_train_scaled_for_feus = pd.DataFrame()\n",
    "        X_test_final = pd.DataFrame() \n",
    "\n",
    "        if not X_train_raw.empty:\n",
    "            scaler_s2 = MinMaxScaler()\n",
    "            X_train_scaled_for_feus = pd.DataFrame(scaler_s2.fit_transform(X_train_raw), columns=X_train_raw.columns, index=X_train_raw.index)\n",
    "            if not X_test_raw.empty:\n",
    "                X_test_final = pd.DataFrame(scaler_s2.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            logging.info(f\"   FEUS Scen2: Después de Escalado. X_train_scaled_for_feus: {X_train_scaled_for_feus.shape}, X_test_final: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "        \n",
    "        if not X_train_scaled_for_feus.empty and not y_train_orig.empty:\n",
    "            X_train_final, y_train_final = feus_scenario2_apply(X_train_scaled_for_feus, y_train_orig,\n",
    "                                                                n_samples_to_keep=N_SAMPLES_FEUS)\n",
    "            logging.info(f\"   FEUS Scen2: Después de FEUS en train. X_train_final: {X_train_final.shape}, y_train_final dist: {dict(y_train_final.value_counts() if not y_train_final.empty else {})}\")\n",
    "        else:\n",
    "            X_train_final = X_train_scaled_for_feus\n",
    "            y_train_final = y_train_orig\n",
    "\n",
    "    logging.info(f\"   → Tamaños finales para modelado. Train: {X_train_final.shape if not X_train_final.empty else '(empty)'}, Test: {X_test_final.shape if not X_test_final.empty else '(empty)'}\")\n",
    "    if not (y_train_final is None or y_train_final.empty):\n",
    "        logging.info(f\"   → Distribución y_train_final: {dict(y_train_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_train_final está vacío o es None.\")\n",
    "    if not (y_test_final is None or y_test_final.empty):\n",
    "        logging.info(f\"   → Distribución y_test_final: {dict(y_test_final.value_counts())}\")\n",
    "    else:\n",
    "        logging.warning(\"   → y_test_final está vacío o es None.\")\n",
    "        \n",
    "    return X_train_final, X_test_final, y_train_final, y_test_final\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Funciones de entrenamiento y evaluación \n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def train_and_save_models(X_train, y_train, exp_name, output_dir):\n",
    "    hyper_file = os.path.join(output_dir, 'hyperparameters.txt')\n",
    "    with open(hyper_file, 'w') as hf:\n",
    "        hf.write(f\"Hyperparameters for experiment {exp_name}\\n\")\n",
    "        hf.write(\"=\"*60 + \"\\n\\n\")\n",
    "    print(f\"> Hyperparameters will be saved to: {hyper_file}\")\n",
    "\n",
    "    if X_train.empty or X_train.shape[1] == 0:\n",
    "        logging.error(f\"X_train está vacío o no tiene características ANTES de entrenar modelos para {exp_name}. Saltando entrenamiento.\")\n",
    "        return {}\n",
    "\n",
    "    n_inputs = X_train.shape[1]\n",
    "\n",
    "    nn_wrapper = KerasClassifier(\n",
    "        build_fn=build_nn_model,\n",
    "        n_inputs=n_inputs,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    specs = { \n",
    "\n",
    "        'nn': (\n",
    "            nn_wrapper,\n",
    "            {\n",
    "                'clf__learning_rate': [0.0001],\n",
    "                'clf__dropout_rate':  [0.05, 0.1, 0.3],\n",
    "                'clf__batch_size':    [16, 32, 64],\n",
    "                'clf__epochs':        [100], \n",
    "                # No 'clf__validation_split' aquí si se maneja en fit_params o se omite\n",
    "            }\n",
    "        ),\n",
    "\n",
    "        'logreg': (\n",
    "            LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "            {'clf__penalty':['l1','l2'], 'clf__C':[0.01, 0.1, 1, 10, 100], 'clf__solver':['liblinear']}\n",
    "        ),\n",
    "        'svm': (\n",
    "            SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "            {'clf__C': [0.5, 1, 5, 10], 'clf__kernel': ['rbf', 'linear', 'poly']}\n",
    "        ),\n",
    "        'rf': (\n",
    "            RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "            {'clf__n_estimators':[100, 150, 200], 'clf__max_depth':[10, 20, 30, None], \n",
    "             'clf__min_samples_split': [2, 5, 10], 'clf__min_samples_leaf': [1, 2, 4]}\n",
    "        ),\n",
    "        'xgb': (\n",
    "            XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "             {\n",
    "                'clf__n_estimators': [100, 150, 200], 'clf__max_depth': [3, 5, 7, 10],\n",
    "                'clf__learning_rate': [0.01, 0.05, 0.1, 0.2], 'clf__subsample': [0.7, 0.8, 0.9]\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, (clf, param_grid) in specs.items():\n",
    "        print(f\"\\n> Entrenando {name.upper()}...\")\n",
    "        logging.info(f\"Entrenando {name.upper()}...\")\n",
    "        \n",
    "        pipe = ImbPipeline([ \n",
    "            ('scaler', MinMaxScaler()), \n",
    "            ('clf', clf)\n",
    "        ])\n",
    "        \n",
    "        y_train_processed = y_train.astype(int) \n",
    "\n",
    "        fit_params = {}\n",
    "\n",
    "        # --- INICIO: Lógica para determinar n_cv_splits y crear cv_obj ---\n",
    "        current_n_cv_splits = 4 # Valor base que tenías, o el que desees (e.g., 5)\n",
    "        \n",
    "        if len(y_train_processed.unique()) < 2:\n",
    "            logging.error(f\"Solo una clase en y_train para {name}. No se puede hacer CV. Saltando GridSearchCV.\")\n",
    "            # ... (código para entrenar modelo por defecto o simplemente saltar) ...\n",
    "            best_params_str = \"CV SKIPPED (una clase)\"\n",
    "            best_score_str = \"N/A\"\n",
    "            best_estimator_for_model = None # Marcar para no guardar\n",
    "            # (continúa al final del bucle para guardar estado y pasar al siguiente modelo)\n",
    "        else:\n",
    "            min_class_count = min(y_train_processed.value_counts())\n",
    "            # n_splits no puede ser mayor que el número de miembros en cualquier clase.\n",
    "            actual_n_splits = min(current_n_cv_splits, min_class_count)\n",
    "\n",
    "            if actual_n_splits < 2:\n",
    "                logging.error(f\"No se puede realizar CV para {name} con n_splits={actual_n_splits} (basado en min_class_count). Saltando GridSearchCV.\")\n",
    "                best_params_str = f\"CV SKIPPED (n_splits={actual_n_splits})\"\n",
    "                best_score_str = \"N/A\"\n",
    "                best_estimator_for_model = None\n",
    "            elif X_train.shape[0] < actual_n_splits:\n",
    "                logging.error(f\"No hay suficientes muestras en X_train ({X_train.shape[0]}) para CV con {actual_n_splits} splits en {name}. Saltando GridSearchCV.\")\n",
    "                best_params_str = f\"CV SKIPPED (X_train pequeño, n_splits={actual_n_splits})\"\n",
    "                best_score_str = \"N/A\"\n",
    "                best_estimator_for_model = None\n",
    "            else:\n",
    "                logging.info(f\"Usando StratifiedKFold con n_splits={actual_n_splits}, shuffle=True para {name}.\")\n",
    "                cv_obj = StratifiedKFold(n_splits=actual_n_splits, shuffle=True, random_state=42)\n",
    "                # --- FIN: Lógica para determinar n_cv_splits y crear cv_obj ---\n",
    "\n",
    "                grid = GridSearchCV(\n",
    "                    pipe, param_grid, \n",
    "                    cv=cv_obj, \n",
    "                    scoring='roc_auc', \n",
    "                    n_jobs=5, verbose=1, refit=True, error_score='raise'\n",
    "                )\n",
    "                \n",
    "                best_estimator_for_model = None # Inicializar\n",
    "                try:\n",
    "                    grid.fit(X_train, y_train_processed)                    \n",
    "                    best_estimator_for_model = grid.best_estimator_\n",
    "                    best_params_str = str(grid.best_params_)\n",
    "                    best_score_str = f\"{grid.best_score_:.4f}\"\n",
    "\n",
    "                except ValueError as ve:\n",
    "                    logging.error(f\"Error de ValueError (posiblemente CV o scorer) durante GridSearchCV para {name} en {exp_name}: {ve}\", exc_info=True)\n",
    "                    best_params_str = \"Error en CV/Scorer\"\n",
    "                    best_score_str = \"Error\"\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error general durante GridSearchCV para {name} en {exp_name}: {e}\", exc_info=True)\n",
    "                    best_params_str = \"Error general\"\n",
    "                    best_score_str = \"Error\"\n",
    "        \n",
    "        # Guardado de modelos y logs (como lo tienes)\n",
    "        if best_estimator_for_model:\n",
    "            # ... (tu lógica de guardado)\n",
    "            if name == 'nn':\n",
    "                if hasattr(best_estimator_for_model.named_steps['clf'], 'model'):\n",
    "                    scaler_nn = best_estimator_for_model.named_steps['scaler']\n",
    "                    keras_model_nn = best_estimator_for_model.named_steps['clf'].model\n",
    "                    scaler_path = os.path.join(output_dir, f\"{exp_name}_{name}_scaler.joblib\")\n",
    "                    keras_model_path = os.path.join(output_dir, f\"{exp_name}_{name}_keras_model.h5\")\n",
    "                    joblib.dump(scaler_nn, scaler_path)\n",
    "                    keras_model_nn.save(keras_model_path)\n",
    "                    logging.info(f\"  ✔ Scaler NN guardado: {scaler_path}\")\n",
    "                    logging.info(f\"  ✔ Modelo Keras NN guardado: {keras_model_path}\")\n",
    "                    best_models[name] = (scaler_path, keras_model_path)\n",
    "                else:\n",
    "                    logging.error(f\"  ✘ No se pudo extraer el modelo Keras para {name}.\")\n",
    "                    best_models[name] = None \n",
    "            else:\n",
    "                model_path = os.path.join(output_dir, f\"{exp_name}_{name}_best_pipeline.joblib\") # Cambiado de _best.joblib\n",
    "                joblib.dump(best_estimator_for_model, model_path)\n",
    "                logging.info(f\"  ✔ Modelo (pipeline) guardado: {model_path}\")\n",
    "                best_models[name] = best_estimator_for_model\n",
    "        else: # Si best_estimator_for_model es None (por error o salto de CV)\n",
    "            best_models[name] = None\n",
    "            if not (best_params_str.startswith(\"CV SKIPPED\") or best_params_str.startswith(\"Error\")): # Evitar doble log si ya se registró error\n",
    "                 logging.warning(f\"No se pudo obtener best_estimator para {name} debido a un error previo no capturado por salto de CV.\")\n",
    "\n",
    "\n",
    "        with open(hyper_file, 'a') as hf:\n",
    "            hf.write(f\"{name.upper()} best params: {best_params_str}\\n\")\n",
    "            hf.write(f\"{name.upper()} best CV ROC-AUC: {best_score_str}\\n\\n\") # O la métrica que uses\n",
    "        print(f\"  ✔ Hiperparámetros (o estado de error) guardados para {name.upper()}\")\n",
    "        \n",
    "    return {k: v for k, v in best_models.items() if v is not None}\n",
    "\n",
    "def evaluate_and_save_reports(models, X_test, y_test, output_dir):\n",
    "\n",
    "\n",
    "    report_file = os.path.join(output_dir, \"classification_reports.txt\")\n",
    "    with open(report_file, \"w\") as rf:\n",
    "        rf.write(f\"Classification reports for {os.path.basename(output_dir)}\\n\")\n",
    "        rf.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    print(f\"\\n> Reports will be saved to: {report_file}\")\n",
    "\n",
    "    if X_test.empty or y_test.empty:\n",
    "        logging.error(\"X_test o y_test están vacíos. No se pueden generar reportes.\")\n",
    "        print(\"X_test o y_test están vacíos. Saltando evaluación.\")\n",
    "        return\n",
    "\n",
    "    y_test_processed = y_test.astype(int)\n",
    "\n",
    "    for name, model_or_paths in models.items():\n",
    "        print(f\"\\n> Evaluando {name.upper()}...\")\n",
    "        logging.info(f\"Evaluando {name.upper()}...\")\n",
    "        y_pred, y_prob = None, None \n",
    "\n",
    "        try:\n",
    "            if name == \"nn\":\n",
    "                if isinstance(model_or_paths, tuple) and len(model_or_paths) == 2: # NN models\n",
    "                    scaler_path, keras_model_path = model_or_paths\n",
    "                    scaler_loaded = joblib.load(scaler_path) # Este es el scaler del pipeline del NN\n",
    "                    keras_model = load_model(keras_model_path, compile=False)\n",
    "                    \n",
    "                    # X_test ya está escalado por run_scenario_...\n",
    "                    # Aplicar el scaler_loaded es para consistencia con cómo se entrenó\n",
    "                    # (especialmente si hubo validation_split interno en Keras)\n",
    "                    X_test_for_eval = scaler_loaded.transform(X_test) \n",
    "                    \n",
    "                    y_prob_all_classes = keras_model.predict(X_test_for_eval)\n",
    "                    if y_prob_all_classes.shape[1] == 2:\n",
    "                        y_prob = y_prob_all_classes[:, 1]\n",
    "                        y_pred = np.argmax(y_prob_all_classes, axis=1)\n",
    "                    elif y_prob_all_classes.shape[1] == 1:\n",
    "                         logging.warning(f\"Modelo NN predijo una sola columna de probabilidad: {y_prob_all_classes.shape}\")\n",
    "                         y_prob = y_prob_all_classes[:,0]\n",
    "                         y_pred = (y_prob > 0.5).astype(int)\n",
    "                    else:\n",
    "                        logging.error(f\"Forma inesperada de y_prob_all_classes para NN: {y_prob_all_classes.shape}\")\n",
    "                        continue\n",
    "                else: # Fallback si model_or_paths no es la tupla esperada\n",
    "                    logging.error(f\"  ✘ Rutas de modelo NN no válidas para {name}. Tipo: {type(model_or_paths)}\")\n",
    "                    continue\n",
    "            else: # Otros modelos (pipeline completo)\n",
    "                model_pipeline = model_or_paths\n",
    "                # El pipeline (scaler + clf) se encarga del escalado de X_test\n",
    "                y_prob_all_classes = model_pipeline.predict_proba(X_test) \n",
    "                y_pred = model_pipeline.predict(X_test)\n",
    "                \n",
    "                if y_prob_all_classes.shape[1] == 2:\n",
    "                    y_prob = y_prob_all_classes[:, 1]\n",
    "                elif y_prob_all_classes.shape[1] == 1:\n",
    "                    logging.warning(f\"Modelo {name} predijo una sola columna de probabilidad. Clases del clasificador: {model_pipeline.named_steps['clf'].classes_}\")\n",
    "                    if model_pipeline.named_steps['clf'].classes_[0] == 1:\n",
    "                        y_prob = y_prob_all_classes[:, 0] \n",
    "                    else:\n",
    "                        y_prob = 1.0 - y_prob_all_classes[:, 0]\n",
    "                else: \n",
    "                    logging.error(f\"Forma inesperada de y_prob_all_classes para {name}: {y_prob_all_classes.shape}\")\n",
    "                    continue\n",
    "\n",
    "            auc = float('nan')\n",
    "            report_str = \"N/A\"\n",
    "\n",
    "            if y_pred is not None and y_prob is not None:\n",
    "                if len(np.unique(y_test_processed)) < 2 :\n",
    "                    logging.warning(f\"Solo una clase presente en y_test para {name}. ROC-AUC no es calculable.\")\n",
    "                elif len(y_prob) == 0 :\n",
    "                    logging.warning(f\"y_prob está vacío para {name}. ROC-AUC no es calculable.\")\n",
    "                else:\n",
    "                    try:\n",
    "                        auc = roc_auc_score(y_test_processed, y_prob)\n",
    "                        if len(np.unique(y_pred)) < 2 and len(np.unique(y_test_processed)) >=2 :\n",
    "                            logging.warning(f\"Todas las predicciones son de una sola clase para {name}, pero y_test tiene variabilidad. ROC-AUC: {auc:.4f}\")\n",
    "                    except ValueError as e_auc:\n",
    "                        logging.error(f\"Error al calcular ROC-AUC para {name}: {e_auc}. y_test unique: {np.unique(y_test_processed)}, y_prob (first 5 unique): {np.unique(y_prob[:5]) if len(y_prob)>0 else 'empty'}\")\n",
    "                \n",
    "                report_str = classification_report(\n",
    "                    y_test_processed, y_pred,\n",
    "                    target_names=[\"No Fraude (0)\", \"Fraude (1)\"],\n",
    "                    digits=4, zero_division=0\n",
    "                )\n",
    "            else:\n",
    "                logging.error(f\"Predicciones (y_pred o y_prob) no generadas para {name}.\")\n",
    "\n",
    "            print(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n{report_str}\")\n",
    "            with open(report_file, \"a\") as rf:\n",
    "                rf.write(f\"{name.upper()} ROC-AUC: {auc:.4f}\\n{report_str}\\n{'-'*60}\\n\")\n",
    "            print(f\"  ✔ Reporte guardado para {name.upper()}\")\n",
    "        \n",
    "        except Exception as e_eval:\n",
    "            logging.error(f\"Error durante la evaluación de {name} en {output_dir}: {e_eval}\", exc_info=True)\n",
    "            print(f\"Error durante la evaluación de {name}: {e_eval}\")\n",
    "            with open(report_file, \"a\") as rf:\n",
    "                rf.write(f\"{name.upper()} - ERROR EN EVALUACIÓN: {e_eval}\\n{'-'*60}\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Bloque principal\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CSV_PATH = \"/scratch/sivar/jarevalo/jsaavedra/creditcard.csv\"\n",
    "    BASE_OUTPUT = \"/scratch/sivar/jarevalo/jsaavedra/resultados_FEUS\" \n",
    "    TARGET_COLUMN_NAME = 'Class'\n",
    "\n",
    "    for scenario in [\"scenario1\", \"scenario2\"]:\n",
    "        exp_name   = f\"FEUS_GlobalSelection_Scaling{N_SAMPLES_FEUS}_{scenario}\"\n",
    "        output_dir = os.path.join(BASE_OUTPUT, exp_name)\n",
    "        log_file   = os.path.join(output_dir, f\"run_{exp_name}.log\")\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        setup_logging(log_file)\n",
    "        \n",
    "        logging.info(f\"=================================================\")\n",
    "        logging.info(f\"=== Iniciando experimento: {exp_name} ===\")\n",
    "        logging.info(f\"Técnica: FEUS (Selección Global con Escalado Controlado), N samples: {N_SAMPLES_FEUS}\")\n",
    "        logging.info(f\"Output directory: {output_dir}\")\n",
    "        logging.info(f\"=================================================\")\n",
    "        print(f\"\\n=== Iniciando experimento: {exp_name} ===\")\n",
    "\n",
    "        df_original = load_and_prepare_unscaled(CSV_PATH) # Carga datos SIN escalar\n",
    "        logging.info(f\"Dataset cargado (sin escalar). Forma: {df_original.shape}. Clases: {dict(df_original[TARGET_COLUMN_NAME].value_counts())}\")\n",
    "\n",
    "        if df_original.shape[0] < 10: \n",
    "            logging.error(f\"Dataset con muy pocas filas ({df_original.shape[0]}). Abortando {exp_name}.\")\n",
    "            continue\n",
    "        class_counts = df_original[TARGET_COLUMN_NAME].value_counts()\n",
    "        if len(class_counts) < 1 : # Permitir si solo hay una clase, FEUS podría manejarlo\n",
    "            logging.warning(f\"Dataset no tiene clases o está vacío: {class_counts.to_dict()}.\")\n",
    "        elif class_counts.get(0,0) == 0 and class_counts.get(1,0) == 0:\n",
    "             logging.warning(f\"Dataset no tiene muestras en ninguna clase.\")\n",
    "        elif len(class_counts) < 2: # Si solo hay una clase, pero tiene muestras\n",
    "             logging.warning(f\"Dataset tiene solo una clase: {class_counts.to_dict()}. El modelado podría fallar.\")\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = run_scenario_feus_controlled_scaling(\n",
    "            df_original.copy(), scenario, target_col_name=TARGET_COLUMN_NAME\n",
    "        )\n",
    "\n",
    "        if X_train.empty or y_train.empty:\n",
    "            logging.error(f\"X_train o y_train vacíos para {exp_name} DESPUÉS de run_scenario. Saltando entrenamiento.\")\n",
    "            continue\n",
    "        \n",
    "        if len(y_train.value_counts()) < 1:\n",
    "            logging.warning(f\"y_train no tiene muestras para {exp_name}. El entrenamiento podría fallar.\")\n",
    "        elif len(y_train.value_counts()) < 2:\n",
    "             logging.warning(f\"y_train tiene solo una clase para {exp_name}. GridSearchCV se adaptará o podría fallar.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        trained_models = train_and_save_models(X_train, y_train, exp_name, output_dir)\n",
    "        logging.info(f\"Entrenamiento para {exp_name} completado en {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        if not trained_models:\n",
    "            logging.warning(f\"No se entrenaron modelos exitosamente para {exp_name}. Saltando evaluación.\")\n",
    "        else:\n",
    "            if X_test is not None and not X_test.empty and y_test is not None and not y_test.empty:\n",
    "                 evaluate_and_save_reports(trained_models, X_test, y_test, output_dir)\n",
    "            else:\n",
    "                logging.warning(f\"X_test o y_test están vacíos para {exp_name} después de run_scenario. Saltando evaluación.\")\n",
    "        \n",
    "        logging.info(f\"=== Fin experimento {exp_name} ===\\n\")\n",
    "        print(f\"=== Fin experimento: {exp_name} ===\\n\")\n",
    "\n",
    "    print(\"Todos los experimentos FEUS con escalado controlado han finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
